{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP_P10_Transformer로 번역기 만들기\n",
    "\n",
    "DATE: MAY forth,2021\n",
    "\n",
    "## Table \n",
    "Step 1. data load\n",
    "\n",
    "Step 2. 데이터 정제 및 토큰화\n",
    "\n",
    "Step 3. 모델 설계\n",
    "\n",
    "Step 4. 훈련하기\n",
    "\n",
    "### step 1\n",
    "DATA: korean-english-park.train.tar.gz file\n",
    "\n",
    "https://github.com/jungyeul/korean-parallel-corpora/tree/master/korean-english-news-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! conda install -c powerai sentencepiece -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from keras.preprocessing import sequence\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from konlpy.tag import Mecab\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import sentencepiece as spm\n",
    "import seaborn # Attention 시각화를 위해 필요!\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager._rebuild()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 94123\n",
      "Example:\n",
      ">> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",
      ">> 북한의 핵무기 계획을 포기하도록 하려는 압력이 거세지고 있는 가운데, 일본과 북한의 외교관들이 외교 관계를 정상화하려는 회담을 재개했다.\n",
      ">> \"경호 로보트가 침입자나 화재를 탐지하기 위해서 개인적으로, 그리고 전문적으로 사용되고 있습니다.\"\n",
      ">> 수자원부 당국은 논란이 되고 있고, 막대한 비용이 드는 이 사업에 대해 내년에 건설을 시작할 계획이다.\n",
      ">> 또한 근력 운동은 활발하게 걷는 것이나 최소한 20분 동안 뛰는 것과 같은 유산소 활동에서 얻는 운동 효과를 심장과 폐에 주지 않기 때문에, 연구학자들은 근력 운동이 심장에 큰 영향을 미치는지 여부에 대해 논쟁을 해왔다.\n"
     ]
    }
   ],
   "source": [
    "path_to_file = os.getenv('HOME') + '/aiffel/aiffel_nlp_ex/data/transformer/data/korean-english-park.train.ko'\n",
    "\n",
    "with open(path_to_file, \"r\") as f:\n",
    "    train_ko_raw = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(train_ko_raw))\n",
    "\n",
    "print(\"Example:\")\n",
    "for sen in train_ko_raw[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 94123\n",
      "Example:\n",
      ">> Much of personal computing is about \"can you top this?\"\n",
      ">> Amid mounting pressure on North Korea to abandon its nuclear weapons program Japanese and North Korean diplomats have resumed talks on normalizing diplomatic relations.\n",
      ">> “Guard robots are used privately and professionally to detect intruders or fire,” Karlsson said.\n",
      ">> Authorities from the Water Resources Ministry plan to begin construction next year on the controversial and hugely expensive project.\n",
      ">> Researchers also have debated whether weight-training has a big impact on the heart, since it does not give the heart and lungs the kind of workout they get from aerobic activities such as brisk walking or running for at least 20 minutes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "path_to_file = os.getenv('HOME')+'/aiffel/aiffel_nlp_ex/data/transformer/data/korean-english-park.train.en'\n",
    "\n",
    "with open(path_to_file, \"r\") as f:\n",
    "    train_en_raw = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(train_en_raw))\n",
    "\n",
    "print(\"Example:\")\n",
    "for sen in train_en_raw[0:100][::20]: print(\">>\", sen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78968"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus = set(list(zip(train_ko_raw, train_en_raw)))\n",
    "\n",
    "len(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, s_token=False, e_token=False):\n",
    "    sentence = sentence.lower().strip() #모든 입력을 소문자로 변환합니다. + 문장 앞뒤의 불필요한 공백을 제거합니다.\n",
    "    sentence = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z?.!,1-9\\\\s]\", \"\", sentence)#알파벳, 문장부호, 한글만 남기고 모두 제거합니다.\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence) #문장부호 양옆에 공백을 추가합니다.\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_corpus = []\n",
    "eng_corpus = []\n",
    "\n",
    "for sentence in cleaned_corpus:\n",
    "    en_sentence = preprocess_sentence(sentence[1])\n",
    "    ko_sentence = preprocess_sentence(sentence[0])\n",
    "    eng_corpus.append(en_sentence)\n",
    "    kor_corpus.append(ko_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78968\n",
      "78968\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(eng_corpus))\n",
    "print(len(kor_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kor.txt', 'w') as f:\n",
    "    for sentence in kor_corpus:\n",
    "        f.write('{}\\n'.format(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eng.txt', 'w') as f:\n",
    "    for sentence in eng_corpus:\n",
    "        f.write('{}\\n'.format(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 토큰화 \n",
    "#https://donghwa-kim.github.io/SPM.html\n",
    "\n",
    "def generate_tokenizer(txt):\n",
    "    templates= '--input={} \\\n",
    "    --pad_id={} \\\n",
    "    --bos_id={} \\\n",
    "    --eos_id={} \\\n",
    "    --unk_id={} \\\n",
    "    --model_prefix={} \\\n",
    "    --vocab_size={} \\\n",
    "    --character_coverage={} \\\n",
    "    --model_type={}'\n",
    "    \n",
    "    train_input_file = txt\n",
    "    pad_id=0  #<pad> token을 0으로 설정\n",
    "    vocab_size = 2000 # vocab 사이즈\n",
    "    prefix = 'botchan_spm' # 저장될 tokenizer 모델에 붙는 이름\n",
    "    bos_id=1 #<start> token을 1으로 설정\n",
    "    eos_id=2 #<end> token을 2으로 설정\n",
    "    unk_id=3 #<unknown> token을 3으로 설정\n",
    "    character_coverage = 1.0 # to reduce character set \n",
    "    model_type ='unigram' # Choose from unigram (default), bpe, char, or word\n",
    "\n",
    "    cmd = templates.format(train_input_file,\n",
    "                           pad_id,\n",
    "                           bos_id,\n",
    "                           eos_id,\n",
    "                           unk_id,\n",
    "                           prefix,\n",
    "                           vocab_size,\n",
    "                           character_coverage,\n",
    "                           model_type)\n",
    "    \n",
    "    spm.SentencePieceTrainer.Train(cmd)\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(prefix + '.model')\n",
    "    \n",
    "    return sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "\n",
    "ko_tokenizer = generate_tokenizer('kor.txt')\n",
    "en_tokenizer = generate_tokenizer('eng.txt')\n",
    "en_tokenizer.SetEncodeExtraOptions(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "for i in range(0, len(cleaned_corpus)):\n",
    "    src_tokens = ko_tokenizer.EncodeAsIds(kor_corpus[i])\n",
    "    tgt_tokens = en_tokenizer.EncodeAsIds(eng_corpus[i])\n",
    "\n",
    "    if (len(src_tokens) > 50): continue\n",
    "    if (len(tgt_tokens) > 50): continue\n",
    "    \n",
    "    src_corpus.append(src_tokens)\n",
    "    tgt_corpus.append(tgt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44559, 50)\n",
      "(44559, 50)\n",
      "[ 128  153   24   44  283   29   28   73  519    4  775   65   56   75\n",
      "  204    4  936   11   12    4  391   53    4  775   65   56   12   75\n",
      "  204   38 1865   36    5    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')\n",
    "\n",
    "print(enc_train.shape)\n",
    "print(dec_train.shape)\n",
    "print(enc_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./kor_spm.vocab', encoding='utf-8') as f:\n",
    "#     Vo_k = [doc.strip().split(\"\\t\") for doc in f]\n",
    "\n",
    "# # w[0]: token name    \n",
    "# # w[1]: token score\n",
    "# kor_word2idx = {w[0]: i for i, w in enumerate(Vo_k)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "def generate_masks(inp, tar):\n",
    "  # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "  # Used in the 2nd attention block in the decoder.\n",
    "  # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "  # Used in the 1st attention block in the decoder.\n",
    "  # It is used to pad and mask future tokens in the input received by \n",
    "  # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1UAAAGhCAYAAACJY57gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdd9hcRfnG8e9N7yAtoJTQO0oVpfcmqEQQUQEVRAVEpShSBKQJilQLooJIl1AUftJBpEivUoWEpqGFloRAwvP7Y+Yky2bP7r7v7tt278917XWy58ycM2c3gfPszDyjiMDMzMzMzMx6Z7qBboCZmZmZmdlQ5qDKzMzMzMysBQ6qzMzMzMzMWuCgyszMzMzMrAUOqszMzMzMzFrgoMrMzMzMzKwFDqrMzMzMzMxa4KDKzMzMzMysBQ6qzMzMzMzMWuCgyszMzMzMrAUOqszMzMzMzFrgoMrMzMzMzKwFDqrMzMzMzMxa4KDKzLqKpLMlvSxp+EC3ZajJn11IOmKg22L1+btqTf7swv+dMLNmOagysyFN0gKSjpf0iKS3Jb0r6T+SzpK0XI0q38/bkZJm7OU1R1U8dDV67d7rm7NekTS8xvfwvqRXJT0u6QJJ35E0z0C3tdNV/VvZv8k680p6r6LeRn3cTDOzls0w0A0wM+stSd8GjgXmAcYC9wICVgG+AewiabuIuKGoExFjJR0K/BY4BDiihSbcBrzaoMxzLZzfWndF3s4AzAssCeycXydIOgk4MiImD1D7usmXgF80UW4E0KsfPMzMBoqDKjMbkiTNAvyKFLTsC1wcEe/lY3MAZ5Mezs6StHTVQ/PvgR8B+0v6VUS83MtmHBoRN/eyrvWDiPhc9T5JK5L+zuwJHAZ8WtI2xd8f6xNjgDUkLRMRTzUo+6W8/R+wUN82y8ysPTz8z8yGqg+Ai4GVI+LPlQ/EEfEO8G1gMjAcWKOyYg6wfgHMAfy4vxpsg0NE/Dsivg1sBYwDNgV+ObCt6nh35O3O9QpJWhjYkNQD/ERfN8rMrF0cVJnZkBQR70XEFyPi7ZLjrzB16N3wGkX+DEwAviVpvr5ppQ1mEXE9qccK0t+DZQayPR3u8rz9Ut1S8EXSs8nlDcqZmQ0qDqrMrJPNkrfTzJeJiDeBa4GZafDrebtUTNpfU9IwSafkfRMlvZCTayxcp/5wSSdL+rekdyRNyH/+uaSFqsrOIGkvSf+U9Hou+x9Jp0tavM41FpH0a0mjc9KP0bmdDQNPSdtL+lvOrviepBclXSRprRpli+x0p0taTNL5kl7L+85udK02OpvUIzIdsFeNdk4naVdJN+b2Tczf2e9LEqEU9fxdfdg/gBeAFSStWqdcEXSd36D9a0o6TdKDkt7K9zBa0pll/4YkfUzSqfl7GJfr3SHpQElzNnMTSs7J9/6gpHmbqWdmnc9BlZl1JEmfABYmDRO8u6TY/+Xt9v3SqKlWAR4m9ZK8DNwMzE5KrnGr0pywD5G0K/A4sB/wUdI93QLMCuwPXFlR9iOkh9jfAGsCj+ayMwF7A49J2q7GNdYAHgK+ldtzS27fvsADwBK1biYHBeeSkkJsDYzOdT8AdgLukPS1ks9i4XwvI/I1/pXr9YuICOCi/HajymP5e7gGOAdYj/T53wbMBnwduE/SVtXn9HdVUwAX5D/X7K2StCSwNvBSblNNkn6Y27EPMF9ux+2khDV7Av+sDpIkLQU8SPp85iR95g8CHwdOqHe9Kr8CdgUeAzaLiNebrGdmnS4i/PLLL7867gVcRXqQu6ROmVVymfHA9D0496hcb6MetqmoN4700L1KxbGPkB7UAvhBVb2tSA+vARwPzFJ1fIPK+yQ9MAdwEzCsYr9ID+qTSUMfl6s4NhvwfK53duU1gE2AV/KxAI6ouv6Jef+jwBpV1/tOPjYRWLbi2NkV53scWKLi2MwtfvfDi3M3WX6bXP59QBX7L8n7/wksXbF/RuCYfOx14CP+rhr+nR8OfCL/+ZmSsj/Ox0/K72+mxr8z4HRScLth1f75SD9WBLB31bFf5/1/p+Lfev4s9wceqCpf3O/win0n5X1PAQu38nfUL7/86rzXgDfAL7/88qvdL+CbTA1elq1TbuaKB+BlenD+URUPXfVel5fUmwAsWeO838rHb67YN11+iAvgtDptUt5+Kpd9A5ivpGzxgHlexb7v5X2PUCPABD5b60Gd1CMyiZTSfnjJ9f6U6/2yYl/lg/qnSuqtTJpb0+h1ZlW94cW5m/w+V6toy9x534b5/bPAPCX1bsll9uv276qJfyvD8/tH8/t1apQtAqI18/ubqR1ULQNMV3K9/XOd86v2X533H1JSb/6q91HV7iKIfhZYtCefgV9++dUdL6dUN7OOImkV4OT89gcR8WRZ2YiYKOl10i/ci5AeiHui0TpV/yrZf0pEPFNj/715u0LFvg2ApUm9Bz8pu1BERP7jTnk7MiJeKyn+W1IA93lJM0bE+8COxbGosWZTRFwh6d/AilWHdgOmJz30jyq53nXAV0lD6KrdEhF31NgPMD8pQGhkdBNl6qlMdjI78CZQDIE7IyLeKKl3Pen7WQ84he7+rpp1PnA0aR7jncVOSSuTguinIuKeeieI+inZi3+PC1Ttf4w03HE7SSdFxISqc5b+O5b0Y1Iv2gvAJhHxfL32mVl3clBlZh0jz6O4hDR35c8R8dsmqhUPV7P34pK9XafqypL9xfyMj1TsWzdv/xnNzd9YPW/vrVPmYdKD/6zASpIeIvXWQJqbUuZRpn1QL9r3KUllGduKB9yP1ThW+gCdP1vVaU+7zF3x5yK4Ke7rs5JqBRgAi+dtcV9d+131QBFU7STpBxFRzMkq5lldULvah0kS8GnSPa0ALJtf8+ci1YsH/4IUVH4SeELSUcC5ETGxwaW+R5obNx7YNCKebaZ9ZtZ9HFSZWUeQND1p3arlgPtIQwCbMWvejuuLdpX4X8n+oteh8oHwo3n7dJPnHpa3r5QViIjJksaSFlYdBszL1M/hxTrnrrU4btG+1ZkaJJSZtca+dxrU6Q+L5O2bFQ/ZxX2VBVSVivvyd9VARDwr6Q7S0MeNgBvzoSIDZ92sfzAlScc5wEp512RSL9LDpPvessZ1X5K0NnAuKRj7HXCMpJNIvZFl97Zf3s4GrA+U9nybWXdz9j8z6xSnk5IEPA9sVz28pxZJM5MeUiE9lPWXnmS3mz5vo26paTUqX3l8loo/v9/D6xTt2zEi1OD1kbpnGjhFD05lBrjivtZq4r5Wq6rj76q+InDaGSAHO0sC90dE3QV/JS1BCsRWIg1V3AiYLSKGR8QmpOQgNUXEMxGxLrAtKfvfgrn8o5I+XVLtNuBzpH+zp+esomZm03BQZWZDnqQDSPNO3gY+ExEvNVl1WdLwsglArTlOg0HRi7Fok+WLXrDqOSVT5F69IpgcA7xVcbjeGkfTpHrP9SE9oA45kmYCdslvr6g41Jv78nfVnItICTNGSJqRJtemyg4E5iIFwFtFxC0RUdkrVz3sbxoRcXVEbAisQ0rNvhhwuaT5axTfLSKuICWqmAX4i6S5a5Qzsy7noMrMhjRJO5DWmZlE+gX+oR5U/1Te3lprwv8gcVferidptibKF/NeplnEtcKqpMyHE4BHI+Itpj5wr1mrgqTpqD1krEjGsUETbRuMDiLNH/oP8OeK/b25L39XTYiIV0hJPuYFNicl3gjgwiaqF8MxL6iYj1VpjR6041+knq7/kgLbbWsUK/67cCSp12op4Jw8p8vMbAoHVWY2ZOVhQ38m9TbtExHX9PAUW+dtWeKIweB60kP03KRf6WvKQxlh6kK2ny/55R1Srx6krHPFELLr8rZsLtqu1O6BKQKRz0taocbxon3TSepNMpA+I2kP0sPyJODbVT0e5+btHpKGTVN56jlmzr1d4O+qJ4peqQNIQe2tEdHMENyiJ2qaIZP5e5rmc88LHtfquSMixjM1QUzpPPP8o8supPT3n611HTPrbg6qzGxIkrQ4KRiaFTixyUx/lfXnArYgZVZr5hfyAZHnhv0ov/2JpMMkVc6rQdL6pF/RiYi7gctID/aXSlqoopwk7QvsSer5OKLiNKeQ5o1sKOn43NtR1Ns+H6/VvoeAPwIzAddUZ8rL19yM1Euyfg9vv+1ywLCupCtIyQomA9+IiOuqiv4NuIE0xO7anPK78jzT517Sh0nzgfxd9cxlpIx6G+f3zQz9A7g/b/eVNGXYpKRijlWtoXmLAP+RdICkeSsPSBpBypI4mQ/PqZtGRDzH1ED2WEkbNtlmM+sCzv5nZkPVYaRsaJOB5eqkiAa4MSJOrdr3FVJGr1PrrBHUyNGS6q1TRT7/jQ3K1BURZ+cHyOOAo4CDJN1Hyli4NGkx1FEVVb5G+mw2AEZJupuUuW1l0gPmBNJQyacrrnFPXo/neOCHwC6SHsnlVyFlPXsGqDVRf29S8LE9cKukJ0nD6WbJ11yA1BvUTJrxtqr4ezEjabjZCkx98L4X+GZE3FddLyJC0s7AX0lzbx7Kn8fzwJykYXlzkwKDdyrq+btqQkS8I+mvwBdJCTf+0mTVY4DtSG19RtI9pCDxk6T1xY4i9T5WmkBaMuFE4HhJDwAvk+ZSFRkED638jOu0+xJJZwF7ABdKWi0iyrJ5mlk3aXaVYL/88suvwfQCziYNAWrmdXZV3elJD5LvAMN6ce1RPbj27jXqDS857/CiXsnxFYBfkRYyHQe8CzwB/BJYqMY9foe0ltHYXPYZ4DfAknXubSvg76T1mt4jLaz7G1Jyg5tz+46oUU+kuTFXkx5YJ5EShzwK/BpYueT7m+Zcbfi7MeVzrHhNJCWGuJ20ZtG6TZ5rBtID9E35M5lEenh/IJ9ncX9XTf1bmebvPCk4CuBvJXWLe9ioav/SpOGZoyo+q1NIaee/kOvcXFVnIeBw4I78GU8iJRa5HFinxrWLvze12j0b8O98/BZghnb/HfbLL7+G3ksRPc38amY2tEnaEzgT+GlEHD7Q7TEzM7OhzUGVmXUVSfOQegxeAj4ZH05OYGZmZtZjTlRhZt3mZNLwpx0cUJmZmVk7uKfKzMzMzMysBe6pMjMzMzMza4GDKjMzMzMzsxY4qDIzMzMzM2uBgyozMzMzM7MWOKgyMzMzMzNrgYMqM+tYkkZI+pekcZJekXS+pMUHul1mZmbWWZxS3cw6kqTvAqcAjwAXAQsAXwcmAGtFxOhenvdZYC5gVHtaamYtGg68FRFLDHRDzKx7Oagys44jaRHgP8BDwAYRMSHv/xRwK3B1RGzfy3O/xkwzzDvjwvPVLTf3uMm9Ob2Z9dDYsWOZYYYZePfddzXQbTGz7jXDQDfAzKwP7AnMBBxWBFQAEXGHpEuBnSQt3sveqlEzLjzfvAv+ZPe6hba9c2wvTm1mPTVy5MiBboKZmedUmVlH2pw0zO/6GseuzNst+q85ZmZm1skcVJlZJ1oJeDQiJtU49mDertiP7TEzM7MO5uF/ZtZRJM1FSiTxYkmRYv9iDc5zb8mh5XvZNDMzM+tQ7qkys04zR96OKzle7J+9H9piZmZmXcA9VWbWaYofi8rS7xX7p693kohYo9b+3IO1eu+aZmZmZp3IPVVm1mnG5+0sJceL/WU9WWZmZmY94qDKzDrNG8BEYFjJ8YXydkz/NMfMzMw6nYf/mVlHiYgPJD1NeUKJIuvfE33ZjqvW+UjDMl7LyszMrDO4p8rMOtGNwIKSVqtxbJuKMmZmZmYtc1BlZp3od0AAx0qa0iMvaWVgd+CuiHhggNpmZmZmHcbD/8ys40TEw5JOBA4C7pB0OTAf8DVgErDXQLbPzMzMOot7qsysI0XED4E9ST8eHQrsRhryt5Z7qczMzKyd3FNlZh0rIs4CzhrodpiZmVlnc0+VmZmZmQEg6WxJL0saPtBtGaok7S4pJN080G0ZiiTdnD+/3Qe6LT3hoMrMzMysQ0naNAdKT0san1//lnSqpCVqVPl+3o6UNGMvrzkqPxQXrw8kvSHpWUlXSTpU0lK9vytrRv7ei+/grz2od29FvSP6sIkdxUGVmZmZWYeRNJ2kvwPXk+aUzgTcBjwALArsCzwiadPKehExljQPdTXgkBabcRtwBfBX4EFgMmlZi58CT0q6SNJ8LV7DmrOlpHkbFZK0DLB6P7Sn43hOlZnZAGlmgWDwIsFm1iszAVuSApqjIuKe4kB+uP4dsAPwZ0lLRsSEirq/B34E7C/pVxHxci/bcGhE3Fy5Q9Iw4OvAgcBOwKclrRcRo3t5DWtsDDAMGEH63uv5Ut7+D1ioLxvVadxTZWZmZtZ5JgF7RMT2lQEVQES8DuwKjCM9OG9WdXwy8AtgDuDH7WxURIyJiOOANYAngEWAy3s71NCackfe7txE2aLMbX3Ulo7loMrMzMysw0TEpIj4fZ3j44DH89uP1ijyZ2AC8K2+GKIXEc8CnwfeBT4BfLXd17ApriMF0BtJKu19kvQJYAXgKeCRfmpbx3BQZWZmZtad5szbaYbeRcSbwLXAzDTXw9FjEfEYcG5++51aZSRtIOliSS9Jek/SGElXStq87LyS5pT0Q0l3Shqb642W9CdJ08wXkrSlpMsl/U/SxHytiyWtX+cas+SEG4/m5B+vSLpE0iqN7lvSqjmJxHP5eq9Jul7STjXKFpkEH5E0q6Tjcr0PJI1qdK1sPGlu23SkIZdliqF/FzRo/6KSDpP0z9z29/P9XyFprZI6s0naPyfBeF3ShJww5Rc9yTQpaY+KxCdrNFuvPzioMjMzM+sykjYCliXNt/lnSbH/y9vt+7ApxQP8apLmrjwg6TjgFmBH4FXgJuAdYDvgWkmHV58sB02PA8eThhg+TuqpeQP4MnCXpDlzWUn6FfB30j3+F7gBeC1f8x+Sjq5xjY8Ad5ISbiwF3A08nNt1D7Bt2c1K2hu4j5Q85F3SovRjgE2AiySV9S5OB1xNmuv2v1xvXNl1ajgvb79U66AkAV/Mb8+v0/4NSD1ZRwGrknq0bgImkj7DW6sD1zy082bg58AypPu/HZgf+AHwuKQFGt2ApC8DvyXd99YRcW+jOv3JQZWZmZlZF5A0d+4lORr4G+lBeM+IeKekyu15u76k6fuoWXcBQXomndLLk4OPHwEvAptHxKoRsWVELEVKuDARODIHh0WdjwHXkIYz/h1YLCI+FRHbRsTHSQ/01zP1+fd7wLfzNdaMiNUiYpuIWAXYAHgdOETSrlVt/j3wcVJwsFREbBgRmwDLkYKML9S6UUnbAqeTgoKdI2LZiNg6IlYENiIFjl+XtFuN6isAawFbRMTaEbEZPcvSd20+/zolPUOfAhYH7o+IJ+qcZ0HgZdKcvPnyvW8BLAH8idSzeXBVnR1y2/8HLBkRm0XEpqT5fDuQgtmZ6zVe0g7A2aRAdNuIuKNe+YHgoMrMzMysw0nah9Rb8yApVfoNwDoRUW/9oidJAc+swJJ90a48t+vt/Ha+3NY5gWOA94HPR8T1VXVGAifkt/tVHPopqffjXuCzEfHfqnrPkHo43pQ0K1D0dO0WEfdVlb2VqUMSj5Y0XW7bJ0hzwSYCO0TEixV1RpN6q6bpQcr1T6643kVV1/sH8MP89nvV9Yt2RMR1FXUmlpSbRkRMAi7Ob2sN5yx6sEp7qbLbgJUj4tyIeL/i/O8Dp+a361bVWTxvH4mIVyvqfBARl5F6vEozTErahtSjORn4XP6sBh0HVWZmZmad7xngKlLvyiRSuvWD6iWhyA/tr+e3i/Rh24qgava8HQHMDVwbEXeX1CmCi/UgzXFiamBweES8V6tSRET+4xbAPMCzEXFDyTX+QhoKuCipJwfSsECAq2qlgY+Il4BzapxrA2Bp4NGIuLzBPX28GKJYYQJTg5beKgKmDwVVuRdyR1IAfVF1pUoR8d+IeKvkcBEwVQ/leyxv11KNBacj4u2y70vSJsCl+e2IyqBysPE6VWZmZmYdLiKuJs3JKdaKOoUUhKwjadU6QwCL9atmLzneDsVcqtfytujpWEpSWQBStGd+STOR5k/NQhoe1syDdzF0rnReTkRMlvQAsClp+NptwJr58O1l9YBHa+wr7mnOOvekiu3CTA02Af4dEePrXLMZtwOjSEHbCjlRCKT5XMOAf0TE882cSNJyud7KpLl5y5KCT5g2vriK9NmtS1pw+iTg1Ih4pcFl1iUNl5wF+EpEXNVM2waKgyozMzOzLhIRY/Kk/2VIwcUPSIkHapk1b3uSFKFpOTnFHPntmLwtUrwvn1+NzFpRZ3TlsLQ6huVtowf74nhR/mN5+2KNsoVavS5F+xbLr0ZmrXpfFvQ2LSJC0gWkOU9fYurwx2aH/iFpfuCPwGcqdo8BniUlPNmlxnU/kLQlcBopQcehwAGS/ggcHxHPlVxuj4o/b8XUZBuDkoMqM7NB7qp1PtKwzLZ3ju2HlphZp8i9MOeRgqqaqcMlzQzMm9++0EdNKXpwXidl0AMokmIcGBE/b+YkFYk0om7BaTUqX318lrxtJnCrVLTvjIjYp4d12+k8UlC1M3B47uX7POl+/lKvoqQZSIlAVidlADyENETzzYoy0wRVMGXu3NdzL9XBpOGG3wZ2k/T9iDizRrX/AV8hpd3/iqTbIuI3PbnZ/uQ5VWZmZmbdqRjqtWjJ8WVJQ9EmkOZk9YWv5e1VETE5/7nosVqwB+cpepQ+ltODN/K/vG2UyrtoQ9GmYj5RvQWR56ixrzf31HYR8SjwELBMXudpa9Lcsmsj4rW6lVOmvtVJc6fWj4hLqgKqGZu4/iMR8WVSKvo/A7MBv5G0cY3ih+b5bruSgtuTB9vaVJUcVJmZmZl1p4XytuxhukjOcGtFwNM2kjYkPahPImX7K/wrbzfowenuIWWHm5Op7W5UHtJcqbL2zQCsVlX+ybxdc9oaU9Q6Z3FP6/ZhevpmFcP8dmLqYsB1F/zN1svb6yNiTI3jTQc8EfF8RHwVuJIUuFenrYf0fZKzP55ASrv+F0nz1ig74BxUmZmZmXUYSftJWq/O8ZmAvfLbG0uKbZ23V7azbfn6nwQuIT2LHlu1NtJfSAknPilp0wbnmRMg95hcnXf/tEiBXqP8dLlH5TrSkMPhkjYvOf2OpOGPzzM1MUWRBOMLtR7uJS3N1ECl0g2k9Zg+Cuze4J7mqne8DS4g9fx8hpSEYzxQljyjUtETNc2Qyfx5n1irUl4suUzRW9poStJhpDXNhgPnNtkb2a8cVJmZmZl1niWAmySdLmnxygM5jfp5wEqkwOK06sr5wX4L0npMF7arUZKWkXQy8A/S0LvfRsRPKstExMvAcfntJZI+W+M8a0u6Bvhyxe6DSQk1NgEulrRQVZ0lSIHXijld/JH50NnVw8okbQCckd/+KCI+yH++CHgJ+AhwoaR5KuosD/yVGgFCvt6P8tvTJe1RHfhJWlHS+Uy7eG5b5cQQ/wRWJCXguDLPeWrk/rzdXtKUnrocNF1MeQ/hHZJOlbRM5U5JyzI1AC1La1+0+X1SQo23gG2AHzfR3n7lRBVmZmZmnedPpJ6IvYG9JT0BPEdKmLA2ad7Pa6RFcmstvPoV0nyXU5uYa1PmaEmvkn7En4e0gHCRPe8lUiKKsoxzR+ey3wQul/Qcab2j6YEV8rEApiQ4iIhHJX2OFPiMAD4r6X7SfKthpKF8k5mayfA0UgbEfYC7JT2Y27UYKVU4pAV3z6+4xjs5c+LVwObAaEl3klK8f5KU8OGvpEWAPyQi/iRpMVKmxd/lz+fBfHip/ALYv+QzaafzmZqgpJmhf5ASRuxHCsb+JeluUtr3dUjZCvdn6gLHld4G9gX2lfQkaX7ePKQhlDOQPss/Nbp4RDwj6dukHwSOknRnnTXG+p17qszMzMw6TETcRwo+vkpaPHVW0kP0OqS1io4DVoiI26rr5jk/+5OCj2NbaMa6wGdJPQsrknrFfg98AVi8TkBFRHwQEXuRApeRpKFnmwKfJg1XOxdYLyIurap3PbAccASpZ2VZUo/bQqRga52IeDqXjYjYN7fvatLQvM2B+YHLgI0j4rAabbuZFKCdR0p1vnG+5lX5nu+rc19Hk4LaP5N6ATcmzR2bnjTscduIOKmsfhtdQgoAxwJ/b6ZCREwgff7HkeaWrQqsQvrsPkl5yvP1SUNNryWtSbYZqZf0LlIK9u0qegIbteF8UgA2HXCBpI81qNJvNHVhaTMza0TSvTMuPmz1BX+y+0A35UOcUt261ciRIwF45ZVXBt0ci6FK0p6kHqCfRsThjcqbmXuqzMzMzCzLc4SOBh7IWzNrgudUmZl1AC8QbGZtcjIpxfUOEfHeQDfGbKhwUGVmZmZmAETE7gPdBrOhyMP/zMzMzMzMWuCgyszMzMzMrAUOqszMzMzMzFrgoMrMzMzMzKwFDqrMzMzMGpA0QtK/JI2T9Iqk8yUtPtDtMrPBwUGVmXUcSU9LijqvOQa6jWY2dEj6LvAXYDbgOOB8YDvgbgdWZgZOqW5mnWku4DbSQ1AtE/uxLWY2hElaBDgRuAfYICIm5P0XArcCpwHb9+K8z5L+WzWqbY01s1YNB96KiCV6WtFBlZl1ormA2yPi5IFuyGDiBYLNemVPYCbgsCKgAoiIOyRdCuwkafGIGN3D887FTDPMO+PC881b6+Dc4yb3vsVm1itjx45l8uTe/dtzUGVmHUXSjMDMgKMDM2uHzYEJwPU1jl0J7ARsAfyuh+cdNePC88274E92r3nQP3CY9b+RI0fy6quvjupNXQdVZtZp5s7bNwa0FWbWKVYCHo2ISTWOPZi3K5ZVlnRvyaHlW22YmQ0eDqrMrNPMlbdvSJqXNLH8jYh4pycn8YOQmUmai/TflBdLihT7F+ufFpnZYOWgysw6TRFUnQeo2CnpcdLwnFNLfnE2M6tWZAodV3K82D972QkiYo1a+/MPN6v3vmlmNpg4qDKzTvMucCrwGPA6aTjgssBuwC+ArSVtExHv1zuJH4TMjKlLz5TNXC/2T98PbTGzQcxBlZl1lIh4HNiver+kw0gp1rcFvgmc0c9NM7OhZ3zezlJyvNhf1pNlZl3CQZWZdYWIeFfSt4DngB1xUGVmjb1BWtduWMnxhfJ2TLsvXLYEgrMCmg1O0zUuYmbWGSLiBeBlYLLAyc4AACAASURBVOGBbouZDX4R8QHwNOUJaoqsf0/0T4vMbLByT5WZdQ1J05GyAT470G0ZrLxAsNk0bgT2lbRaRNxfdWybijJm1sXcU2Vm3WQbYE7gloFuiJkNGb8DAjhW0pQfoyWtDOwO3BURDwxQ28xskHBQZWYdRdIJklaosX8Z4HTSxPNf9XvDzGxIioiHgROBrYA7JB0i6STgVmASsNdAts/MBgcP/zOzTrM+cICkW4A7SGnVlwR2Jf03b5eIeG4A22dmQ0xE/FDSU8DewKGkH2duBA7JGUfNrMs5qDKzTrMt8F3gM6QHoNlIySlGAsdHxL8HsG1mNkRFxFnAWQPdjnrzHj3f0WzgOKgys44SEa8DR+SXmZmZWZ/znCozMzMzM7MWOKgyMzMzMzNrgYMqMzMzMzOzFnhOlZmZ9UgzCwSDJ82bmVn3cE+VmZmZmZlZC9xTZWZmZtYBynqR3Wts1vfcU2VmZmZmZtYCB1VmZmZmZmYtcFBlZmZmZmbWAgdVZmZmZmZmLXBQZWZmZmZm1gJn/zMzMzPrYPXWlnNmQLP2cE+VmZmZmZlZC9xTZWZmfaLer+MF/0puZmadwD1VZmZmZmZmLXBQZWZmZmZm1gIHVWZmZmZmZi1wUGVmZmZmZtYCJ6owMzMz61JlCWWcRMasZ9xTZWZmZmZm1gIHVWZmZmZmZi1wUGVmZmZmZtYCz6kyM7MB4wWCzcysE7inyszMzMzMrAXuqTIzMzOzD6nXi+zeY7NpuafKzMzMzMysBQ6qzMzMzMzMWuCgyszMzMzMrAUOqszMzMzMzFrgoMrMhhRJq0p6WVJI2qikzAySDpb0pKR3JY2W9DNJs/Zzc83MzKwLOKgysyFD0i7ATcACdcoIuBA4FngKOBK4HTgQuE7SjP3QVDMzM+siTqluZkOCpAOAE4HLgBeBfUqK7giMAM6IiCllJN0HnADsC5zUt621dvICwWaDi9Otm03LPVVmNlQ8CWwWETsAr9UptzcwETi0av9JwEuUB2NmZmZmveKgysyGhIi4MiJuqFdG0uzAp4F/RMQbVfUnA1cDS0hapu9aamZmZt3Gw//MrJMsS/rv2oMlx4v9K5LmW5WSdG/JoeV71zQzMzPrVO6pMrNOsmjevlhyvNi/WD+0xczMzLqEe6rMrJPMkbfjSo4X+2dvdKKIWKPW/tyDtXrPm2ZmZmadyj1VZtZJiv+mTS45Xuyfvh/aYmZmZl3CPVVm1knG5+0sJceL/WU9WWZm1oKydOtOtW6dzj1VZtZJxuTtsJLjC1WVMzMzM2uZe6rMrJM8kbdlGfpWrCpnHcILBJuZ2UByT5WZdYyIeBV4GNhE0kw1imxNWji4LOW6mZmZWY85qDKzTnMmMD9wYOVOSd8AVgD+kBcCNjMzM2sLD/8zs05zJrATcLSk1YG7gJWALwOPAscMYNvMzMysAzmoMrOOEhHvSdoKOAz4IvAZ4GXgDODwiHhzINtnZtaN6s179HxH6wQOqsxsyImII4Aj6hwfDxycX2ZmZmZ9ynOqzMzMzMzMWuCgyszMzLqOpFUlvSwpJG1UUmYGSQdLelLSu5JGS/qZpFn7ublmNsg5qDIzM7OuImkX4CZggTplBFwIHAs8BRwJ3E7KLHqdpBn7oalmNkR4TpWZmXWFZhYIBk+a73SSDgBOBC4DXgT2KSm6IzACOCMippSRdB9wArAvcFLfttbMhgr3VJmZmVk3eRLYLCJ2IC0GXmZvYCJwaNX+k4CXKA/GzKwLuafKzMzMukZEXNmojKTZgU8DN0XEG1X1J0u6GthD0jIR8VQfNbVrlPUiu9fYhhL3VJmZmZl92LKkH54fLDle7F+xf5pjZoOde6rMzMzMPmzRvH2x5Hixf7FGJ5J0b8mh5XvaKDMbvNxTZWZmZvZhc+TtuJLjxf7Z+6EtZjYEOKgaIEpulPSEpOZSUtkUkm7Oa4vsPtBtGWokDc+fXQx0W8zMBqni+WhyyfFi//SNThQRa9R6AY+3o6FmNjg4qOoDklaT9Ep+cD27VpmICGBPYHHg3BauFT14bdTb61htVZ/viCbrrFxVb3jfttLMzHpofN7OUnK82F/Wk2VmXcZzqtpM0lbABcA8jcpGxH8knQQcLGn3iDi7hUtfx9T/CZR5tYXzW2NfAi5tspyZmQ1eY/J2WMnxharKWR+ot7acMwPaYOOgqk3yyuoXAjsAbwD/BNZrouoJpLUwjpJ0QURM7GUTvhkRo3pZ11o3BthW0pwR8XaDsjsDk0h/T+bv85aZmVlPPZG3ZckkVqwqZ2ZdzkFV+8wOfB4YCXwP+AZNBFUR8YakM4EDgG8DJ/dlI63P3AF8Lr9Kh3NK+iSwJHAvKaByUGU2yNT7dbzgX8k7W0S8KulhYBNJM0XEe1VFtiYtHFyWct3MuoznVLXPO8CKETEiIp7vYd3f5e2PJDnQHZouz9tGQ/uK45f1YVvMzKx1Z5J++DqwcqekbwArAH+IiLJEFmbWZRxUtUlETIqIXmXyiYgnSb92DQO2bGvDSlQkSZhf0tKS/iDpRUnvSnpG0kmS5q5Tf2VJZ0p6WtIESeMk3S/pcElzVZWdVdJBku6R9GYu+5ik4yWV9tRIWkHSuZJeyu16StJRkmZrcG/TSdo1Z1d8TdJESaMk/V7ScjXKF5kED5C0iqS/Snor7zuiiY8TUg/lu8DmkuYraxewU357QYN72FjSWflzGpfv4SlJP5c0Z0mdZSWdLenJ/J2MlXSTpL0kzdTMTUiaWdJ1+d6vl1Q2SdvMrNOdCdwKHC3pUkk/lPSnvP9R4JgBbZ2ZDSoOqgaP/8vb7fv5upsC9wNfBZ4hzQUbBnwfuDoHAh8i6cekIHBPYK5c53bSxN0jgbMqyi4GPAD8jDQ2/b5cfj7gh8DjeUhc9TW2zWW/AgRwMyloOQy4E5i31s1ImgO4BjiHNPzyceA2YDbg68B9OZlILSvnc29EGs73QL52Q3ke1V9JQ2q/UFJsQ2Bh4M6IeKbsXJJ+DdxIGkI6E+l/6ncDiwD7A9dJmr6qzjqk72Q30r/rm0hj/dcFfkOa71dXnhf4F2Az4B/A9hHxbqN6ZmadKA/52wo4HlgNOArYGDgDWC8i3hzA5pnZIOOgavC4PW836ufr/gF4GFg6ItaPiM1IwxpeAT5NmiM0haS9SL/OBWnu2EcjYvOI2Bz4KGle2eu57AykYW7LAhfnshtHxJa57LGk4OrKyh4rSR8lBQGz5DKLRcRWEbEKafjcUsAqJffzR1JQcBtpOOa6EbEJ8LF8rtmA81V7bbDdSAHmkhGxZUSsRvqfabPOz9udS44XQ//q9lIBiwJ/A1aPiKXyva8HLEdKiPFJ0v/oK/2E9HmdFRFLR8Q2EbEOKUA+Fpi13gVzkHY+8BlSYPmZiGiUTdLMbEiLiCMiQhFxc8nx8RFxcEQsGREzR8SiEfHdiHijn5tqZoOc5+8MHkUGoaUlzdKLHoJnJdU7fkpEfK/G/nHANpX/g4iI5yT9gdSTtD1paBt52NnPcrEfRsQplSfKa29dLumKvGtHYHVSD9hXKyf6RsQk4BBJq5Em/H4fOCQfPoi0mv01EVHsK+pdKGkYNRJ6SNqQ1Es0ihQUVN7T+/l66wEbALsCp1Sd4j3gyxHxSkW9nmRjvBoYC2wg6aMR8VJF22YERpAWjLyowXn2j4hpMkrl7+USYB9SD9RVFYcXz9ubq+qMJd13vWGW05GC0S+Qege3biKDoZmZ2YApSyjjJDI2UNxTNXgUD+DTkYaI9dR1wBV1Xg+X1Du85Be3e/N2hYp9I4C5gf8ybUAyRQ6uYOr8oXNqZE4qnFlVFlIwBmmIRS2/AmoNu/haUa/Or4jX522tzIwXRMToknoN5Xu8lPQd7lR1eEvSkMWbIqLuuia1AqoKRcC3QNX+x/J2RK1kJxFRb42yX5OGfz4MbOFfYM3MzMx6xj1Vg8eEij/P3ov6vV2n6sqS/a/nbeVPQevm7dW5p6mR1fP23jpl7snbpSXNQxqm9tG8745aFSLifUlPAmtVHSra99ncI1VL0aPzsTptacX5wB6koX6VvWlfqjjeUA6MNiIN9VueNIRyOVJQCzBjVZWjgG1Iwy8fknQ4cFmjzFSSTga+SQrqN4uI15ppn5mZmZlN5aBq8Kic8zKuH6/7v5L9xcN4Zda4Ith5uslzFyvRv1KnTOWxYUCR2W5ig96VWj1fRfuaWXS51hyjd5qo18gtwIvA2pKWjIhnJM1KGkY5kTyUsh5JW5DS7C+Wd70PPAfclds9zf1FxIOS1gX+BKwEXEIaEvoz4I91egr3y9uFgI+TejzNzMzMrAccVA0eRUDwAeWBTttFxAc9KF5knGsqI17lZXpwrEjh/X4PrwFT27dWRLSj16nHIuIDSReSsvTtTEoSsR1pjthljbJF5UyIfyP1RF1A6u26t+hxkrQ7JUFjRNwnadV83QOBT5Ay/+0jaceSlP+XkeZmnUVK4LFaRLzQs7s26z5eINjMzCp5TtXgUayf9HRETKhbcuAUvUqLNlm+CA6r5/9UWrDiz2OAt/KfZ5c0c516c9TYV8xVWrDGsf50Xt5+qWrbzNC/n5ACqnMjYpeIuKtqCF/1sL8PiYgPIuL8nLlwS+ApUqr4y0vWqtoxIn4PnEta5PKSnFTDzMzMzJrknqrB41N5e/NANqKBu0jrRm3eZPl7SHOY1iJlxqulmBf1n4h4Q9J7pN666YA1mJpqfoqchXD5Guf6F2nI3AZ1rtfnIuJ+SY8DK0tam5Td8G1SD1QjRS/UeSXH1+hBO66VtAEwmhS0f4o0PLGyTBGwfScfXwf4OVOHBZqZmQ0Z9XqR3Xtsfck9VYPH1nlbljhiMLiUNC9oWUm7lRWq6GEqUofvVqfX6Vt5eyGkNUGYGkjtVVJnf6DW+c7N2z1y2vXS9pX02rRT0St1PKmtlzWZJr/oJZpmyKSklZia4bBy/2x17mcM6TuDOj+iRMQ7pB6194HvSvpiE201MzMzMxxUDQqSliHNfxkDXDPAzSmV110qFsP9raS98qKxU0j6LFMDw0tJvVVLAH+WNFdFuRkkHQdsAbwM/KLiNCfl7a6S9q46/zeBg0ua+DfgBtKCwtdKWrmq7vSSdiClDl+yiVtuRRFUbVz1vpH78/YgSVOGOOZshlczdd5YpbWBxyV9s7JOth8p+cc7pJ7GUnkeWrEu2FmSavUGmpmZmVkVD/8bHPbM2581maq8ljMljW9Q5tCIeKSX5y8cCcxDelj/DXCMpAdJPRwrkuZb3QxTkjZ8nrQ21BeArSXdlcuuRppr9Rppod4pffIRcZmkXwPfBk6XtB8p4+AywNLAbaRArUjuUdQLSTsDfyUNY3tI0iPA86TAYlVSSvLxtCfTX6mI+I+kf5FSor9CCvaacRjwd2BTYLSk+0hp7VcnDeM7mdRTV+lNYBHgt8Cpku7P+5YGliINp/xWkwv6/hzYjBTsXipp7Yjoz2yUZmZmZkOOe6oGmKS5ScPcXiAtwtpbmwOfbfCav6XGkgKXiPgeKWg5h/Tw/mlgQ+AN4Kf5WkX5F0gB1I+BJ4E1gfWBsaQeqRUj4u4a1/kO8GXgVlLiiU3zoRNIQyVrZgfMadjXJwWqt5DWo9qSlC58VMU1+yPDXTEv6uJmg+WIuIHU83QFae2y9Ujrlh1LuodpMkNGxP2kRZpPAP5NSqm+KTBbbsMqEVE2R6v6XAHsSuo9XJGpizObmZmZWQmlZygbKJKOIQUc34iIPwx0e8wGu5w2/npST+fGEXFz1fFFSL2TZe6NiDVbuP69My4+bPUFf7J7b09hXcKT4vvHyJFp+b9XXnlFA9yUHvF/S/qf/01aIyNHjuTVV1+9LyKaTgxW8PC/ASRpSeAHwNUOqMwak7QLcBowb51ixdy9c4AHahwfU2OfmZmZWa85qBogkkRacPV5UppyM6tD0gHAiaQFi18E9ikpWgRVF0fEgKXWN/MCwWaDi9OtW1/ynKoBkucmbRIRy1YmaTCzUk8Cm0XEDqQEJ2WKoMr/rszMzKxfuKfKzIaEiGh2DbciqHqjr9piZmZmVslBlZl1miKoelvSgqT/zr0WERPr1JmGpHtLDnn9LjMzM/sQD/8zs05TBFXPkZJSvAi8I+nWvDi1mZmZWVu5p8rMOs1o0rpezwBvAcNI66PtDFwu6ciIOKLRScrSqeYerNXb1lozMzMb8hxUmVlHiYjLSBkCP0TS4cDNwGGSLo2Ih/u7bWZmZtaZ2hpUSRoBHASsDIwHrgMOjojRTdafDTgC+CLp1+XRwB+BEyNicjvbambdJSKek3QocB4wAnBQZWZmQHm6dadat2a1LaiS9F3gFOAR4DhgAeDrwGaS1moUWEmaGbgB+CRwEfAQsF4+12qkQKu3bXuWNM9iVG/PYWZtNxx4KyKW6Mdr3pe3C/fjNc3MzKzDtSWokrQIaVHOe4ANImJC3n8hcCtwGrB9g9PsB6wDHBgRP6849xnAdyRdFBEje9nEuZhphnlnXHi+eXtZ32xQm3vc0OvIHTt2LJMn93u758jb1/v7wma1eIFgM7PO0K6eqj2BmYDDioAKICLukHQpsJOkxRv0Vn0HeAn4ZdX+Q4FvAPsAvQ2qRs248HzzLviT3XtZ3WxwG4oPXSNHjuTVV18d1c+X3Tlvb+nn65qZmVkHa1dK9c2BCcD1NY4VC3ZuUVZZ0rLA4sBV1XOnImIsqbdrvTznysysJklzSDpd0nw1jm1D6hF/CLi23xtnZmZmHatdPVUrAY9GxKQaxx7M2xUb1K8sW+scmwHL1CljZibgm8DXJf0fKYCaAKwN7EBas2qniPhg4JpoZmZmnabloErSXKQkEC+WFCn2L1bnNItWla13jtKgKq8fU8vyda5tZh0iIt6W9HFSj9RGwNbA9KQkNScCJ0TEawPWQDMzG1LqzXscikPvre+0o6eqmPg9ruR4sX/2Pj6HmXWJvHjvESXHHgO+1Z/tMTMzs+7WjqCqmJdVlsar2D99H5+DiFij1v7cg7V6vbpmZmZmZma90Y5EFePzdpaS48X+sl6odp3DzMzMzMys37UjqHoDmAgMKzm+UN6OqXOO4lgr5zAzMzMzM+t3LQ//i4gPJD1NeTKIIuvfE3VOUxxrdI4ne9g8MzOzIa2ZBYLBk+bNzAZSu9apuhFYUNJqNY5tU1GmzP3A68BW1QckzQpsDDzorF1mZmZmZjbYtGudqt8B+wDHStquWK9K0srA7sBdEfFA3ncSsA7w7Yh4ECAiJkv6I7C/pC9HxHkV5/4x8BHg0Da11czMzMysJWW9yO417k5tCaoi4mFJJwIHAXdIuhyYD/gaMAnYC0DSAsD3c7U9SYFY4WjgM8A5kjYHHiMFX58DbiIFbmZmZmZmZoNKu4b/ERE/JAVKM5B6lXYjDflbq+ilAl4FriElt7iyqv4bwLrAmcBmwFHAqsBPgW0i4v12tdXMzMzMzKxd2jX8D4CIOAs4q87xoMa8qYrjrwHfyS8zMzMzM7NBr209VWZmZmZmZt3IQZWZmZmZmVkL2jr8z8zMzMysm9VbW86ZATuXe6rMzMzMzMxa4J4qMzOzDlDv1/GCfyU3M+sb7qkyMzMzMzNrQVuCKkmzSTpc0qOSJkh6W9IdknZtsv4ikqLO6552tNPMzMzMzKzdWh7+J+njwBXAR4GrgfOBeYBdgHMkLRoRxzQ4zVx5ew7wQI3jY1ptp5mZmZmZWV9ox5yq1YAXgC0j4olip6QTgceBH0v6RUS8W+ccRVB1cURc3YY2mZmZmZmZ9Yt2BFXXA+dFxPuVOyPiZUnXADsDKwD31zlHEVR5Bq2ZmZmZdaSyhDJOIjP0tRxURcQLdQ5PaPI0RVD1RovNMTMzMzMz61d9llJd0vTAJqTA6okGxYug6m1JC+Z2vRYRE/uqfWZmZmZmZu3Ql+tU7QssDpwWEeMblC2CqucA5T9PknQn8POIuKKZC0q6t+TQx9//72u8fOTZzZzGbMgZOW7yQDehx8aOHQswfICbYWZdRtJswAHAF4ElgUnAI8CvI+JPVWVnAA4EvgYsRkqcdSFwREQ0OxrHzLpAnwRVklYAjgGeBw5vospo4FjgGeAtYBiwJmk+1uWSjoyII1po0mTem/Tm+6PHjMrvl8/bx1s4pzXPn3cfe/XDb4fK5z2c9O/dzPpJty8Q3JOMxZJECqBG5LJ/BFYlBVnrStq4ej65mXWvtgdVkmYFLgZmAr4cEQ3nSUXEZcBlNc51OHAzcJikSyPi4QbnWaPJNt7bk/LWGn/e/cuft5lZqZ5kLN6RFFCdERH7VJS9DziBNCLnpP5svJkNXm1Z/LeQf9X5I7AycFBE3NrK+SLiOeBQUjtHtN5CMzMz62LXAxtXBlSQMhYD1wCzkTIWA+wNTCQ9h1Q6CXgJ2Aczs6ytQRXwU9IY5T9ExC/bdM778nbhNp3PzMzMulBEvFBnyN6UOVKSZgc+DfyjesRNREwmDQdcQtIyfdZYMxtS2jb8T9JXgUNIw/W+1a7zAnPk7ettPKeZmZkZUDNj8XKkZ6QHS6oU+1cEnmpw7rIkWsuX7DezIagtPVWS1gfOAp4ERrR54ubOeXtLG89pZmZmVigyFp+VMxYvmve/WFK+2L9YXzfMzIaGloMqSUuTkky8A3wmIkp7lCQdKOluSZtW7JtD0umS5qtRfhtgP+Ah4NpW22pmZmZWqSRjcTFKZlxJtWL/7I3OHxFr1Hox+DO0mlkPtGP433nAfMBfgG1Tropp3BkRdwJHkCaBfh+4IR8T8E3g65L+jxRATQDWBnYg/Rq0U0R80Ia2As6K1t/8efevTvy8va6MmfWFOhmLix+dyxYBLPZP34fNM7MhpB1B1bC8/UJ+1XIkcCdwAemh6C/FgYh4O68bsR+wEbA16T9So4ATgRMi4rU2tNPMhiCvK2NmfaEqY/EPqjIWj8/bWUqqF/vLerLMrMu0HFRFxPAelN0D2KPG/sdob3ILM+scXlfGzPpCvYzFY/J2GLUtVFXOzLpcu1Oqm5m1m9eVMbO2aiJjcfHfm7IMfStWlTOzLuegyswGNa8rY2bt1EzG4oh4FXgY2ETSTDVOszXwGuUp182sy7RtnSozs/7Ul+vK5PN7bRmzDtOTjMXAmcBppDmZx1Sc4xuk3vET8w82ZmYOqsxsyCrWlTktIsZL8royZtZITzIWnwnsBBwtaXXgLmAl4MvAo1QEWmZmDqrMbMjp63VloDw1fe7BWr25lprZINN0xuKIeE/SVsBhpIQWnwFeBs4ADo+IN/u6sWY2dHTlnCpJIyT9S9I4Sa9IOl/S4gPdrqFO0qqSXpYUkjYqKTODpIMlPSnpXUmjJf0srxViTZA0m6TDJT0qaYKktyXdIWnXGmU77vP2ujJm1lsRMTwi1OB1REX58RFxcEQsGREzR8SiEfHd6nmbZmZdF1RJ+i6p23824DjSmjfbAXc7sOo9SbsANwEL1ClTrCF0LGlOy5HA7aTx6tdJmrEfmjqk5TWb/k3KbvcUcDTwG9KQtnMkHVJRtuM+76p1ZQ7yujJmZmY2GHTV8D9Ji5AWFL4H2CAiJuT9FwK3kiakbj9wLRyaJB1A+lwvI81bKUtb7TWEWtftazZ5XRkzMzMbdLqtp2pP0pChw4qACiAi7gAuBbZzb1WvPAlsFhE7kFLMlvEaQq3r2jWbvK6MmZmZDVbdFlRtTkq/fH2NY1fm7Rb915zOEBFXRsQN9cp4DaH26NY1m7yujJmZmQ1m3RZUrQQ8GhGTahyrXMPG2m9Zml9DyHqoxppNHfN592JdmflJ88Yqz1GsK/MHrytjZmZm7dY1c6okzQXMhdewGSheQ6hvdfKaTV5XxszMzAa1rgmqaPMaNtZj/vz7SH+s2TTAvK6MmZmZDWrdFFR5DZuB5c+/D3TDmk0RMbyH5ccDB+eXmZmZWZ/rpqDKa9gMLH/+bVa1ZtMPvGaTmZmZ2cDopkQVb5DSS3sNm4HhNYTaz2s2mZmZmQ0CXRNURcQHwNN4DZuB4jWE2shrNpmZmZkNHl0TVGU3AgtKWq3GsW0qylibeQ2h9vGaTWZmZmaDS7cFVb8DAjhW0pT5ZJJWBnYH7oqIBwaobd3Aawi1yGs2mZmZmQ0+3ZSogoh4WNKJwEHAHZIuJ61/8zVgErDXQLavC3gNodZ5zSYzMzOzQaargiqAiPihpKeAvYFDSVnSbgQOiYjHB7RxHc5rCLWF12wyMzMzG2S6LqgCiIizSHNSrM0i4gjgiDrHvYZQC7xmk5mZmdng021zqszMzMzMzNrKQZWZmZmZmVkLHFSZmZmZmZm1wEGVmZmZmZlZCxxUmZmZmZmZtcBBlZmZmZmZWQscVJmZmZmZmbXAQZWZmZmZmVkLHFSZmZmZmZm1wEGVmZmZmZlZCxxUmZmZmZmZtcBBlZmZmZmZWQscVJmZmZmZmbXAQZWZmZmZmVkLHFSZmZmZmZm1wEGVmZmZmZlZCxxUmZmZmZmZtcBBlZmZmZmZWQscVJmZmZmZmbXAQZWZmZmZmVkLHFSZmZmZmZm1wEGVmQ160v+3d6+xlpX1Hce/vzBOuUkNUMALNyOEAaIBW5NqMEKqiIJtHAQialS0agBrbEeDMCMEQYGIaS3GIEJ5AaEURXgxYqGUSstNUUc6lQGtjoLNjCDXuXAZ/32x1obj8ezDObPm7H327O8nOXnOfp5nrfOftXdOzm/WWs/K9kmWJVmZZEOSJ5LcnuR9k+a9IklN8/X9Yf0bJEnS1mvBsAuQpOkkeQ1wHfAyYDlwJfAS4N3A5Un2rKpz2uk7te3lwI+m2N2aOS5XkiSNIUOVpPnuEOAB4MiqWtXrTHIBcC/wmSRfrKqNPB+qrq6q5YMvVZIkjSMv/5M0390EHD4xUAFU1VrgO8D2wKK2uxeqHhlceZIkadx5pkrSvFZVD0wzvGHS616oenSOypEkSfoDDuMIawAAEONJREFUhipJIynJNsARNMGqdxarF6qeSLIbze+4h6vqqc3Y/919hg6Y7b4kSdLWzcv/JI2qU4G9gUuqan3b1wtVv6RZlOJB4Mkktyb5yyHUKEmSxoBnqiSNnCSLgHOAXwHLJgytBs4F/hd4HNgd+FPgBOBbSc6qqjNn8jOq6rV9fvbdwKGbXbwkSdrqGKokjZQk2wFXAwuBE6vqufunqupa4NoptlkG3AIsTfKNqrpnQOVKkqQx4OV/kkZGkgCXAQcDn6qqW2eyXVX9EjiD5nfe4rmrUJIkjSNDlaRRcjZwPHBpVX1pltv+oG1fumVLkiRJ485QJWkkJHkvcDrNZXwf3Yxd7Ni2v91SNUmSJIGhStIISHIYcAlwH7C4qp7ZjN2c0Lb/scUKkzRykhyS5OtJfpZkY5IHktyY5Pgp5i5IclqS+9q5q5Oc197bKUnPMVRJmteSvIpm8YkngaOrasozTUl2TPKPSXaZYuxtwN8APwb+dS7rlTR/JTkS+D7wV8B3gTNpFr55NXBVks9OmBvgKpoVRe8HzgJuA5YANyZ50UCLlzSvufqfpPnuCmAX4Brg7c3fOX/gDmAl8NfAB5N8myZAbQBeB7yT5plVx1XV7wZRtKR5aQ/gH4ClVfVkrzPJucAK4IwkX62qNcC7aBa2uaiqTpkw9wfA+TTPyrtwkMVLmr8MVZLmu93b9tj2aypnVdUdSV5Dc0bqTcBRwDbAL4ALgPOr6uG5LVXSPHdFVV0+ubOqHkpyPc39mocC3wZOBp6iWTl0oguBTwCnYKiS1DJUSZrXqmqfWcz9CZu3iIWkMVBVz04zvK5tn0iyA/B64N8nPguv3cemJMuBDyXZr6run6NyJY0QQ5UkSRprSV4MHAP8BvghsD/N30gr+mzS6z+Q5n6r6fZ9d5+hA2ZfqaT5ylAlSZLGTpIdgVfSLFLxSWAfmvsu1yXZs532YJ/Ne/17zWmRkkaGoUqSJI2jY4HL2u/XAEdW1S3t695z7dZN3mhS/w4v9EOq6rVT9bdnsA6dUaWS5j2XVJckSePoZuA9wDJgPXBTkiXtWO/vo019tu31bzN35UkaJZ6pkiRJY6eqfknzyAaSfAG4FTg/yZ00IQtg2z6b9/r7ncmSNGY8UyVJksZaVT0DnNO+XExzOSA8/0iHyfZo2zV9xiWNGUOVJEkS/LxtXw6sar/vt0LfgW27qs+4pDFjqJIkSWMhya7TDC9q219X1UPAPcARSRZOMfco4GH6L7kuacwYqiRJ0ri4PsnHkvzeAhNJdgbObl9e1bYXA7sCSybNPYkmgF1aVf0WspA0ZlyoQpIkjYsVwFeATydZDqwGXgqcQHP/1Oer6rZ27sXAccDnkhwK3AUcBJwIrOT5e7AkyVAlSZLGQ1V9LMl1wAeBo2mC1AbgbuAjVXXdhLlPJ3krsBQ4vp2/FrgIWFZVjw26fknzl6FKkiSNjaq6AbhhhnPXA6e1X5LUl/dUSZIkSVIHhipJkiRJ6sBQJUmSJEkdGKokSZIkqQNDlSRJkiR1YKiSJEmSpA4MVZIkSZLUgaFKkiRJkjowVEmSJElSB4YqSZIkSerAUCVJkiRJHRiqJEmSJKkDQ5UkSZIkdWCokiRJkqQODFWSJEmS1IGhSpIkSZI6MFRJkiRJUgeGKknzXpJDknw9yc+SbEzyQJIbkxw/xdwFSU5Lcl87d3WS85JsN4zaJUnS1m/BsAuQpOkkORJYDjwKXA+sAnYDTgSuSnJAVZ3Vzg1wFbC43eYy4NXAEuANSQ6vqmcG/6+QpD+wzzP/9zBrz/qnYdeheeCb6zYNuwQBjzzyCMA+m7OtoUrSfLcH8A/A0qp6steZ5FxgBXBGkq9W1RrgXTSB6qKqOmXC3B8A5wOnAhcOsnhJ6uNxnn6WZ1av+UX7+oC2vXdI9WiIHvr9l34Whmcf4PHN2dBQJWm+u6KqLp/cWVUPJbke+ChwKPBt4GTgKeCMSdMvBD4BnIKhStI8UFX7Tnyd5O62/7XDqUjzhZ+F0eQ9VZLmtap6dprhdW37RJIdgNcD362qRyftYxPN5YD7JtlvbiqVJEnjyjNVkkZSkhcDxwC/AX4I7E/zO21Fn016/QcC989g/3f3GTqgT78kSRpThipJIyPJjsAraRaf+CTNtc/HVdW6JHu20x7ss3mvf685LVKSJI0dQ5WkUXIszYp+AGuAI6vqlvb1jm27bvJGk/p3mMkP6ncte3sG69CZ7EOSJI0H76mSNEpuBt4DLAPWAzclWdKO9X6f9VuXtte/zdyVJ0mSxpFnqiSNjKr6JXAFQJIvALcC5ye5kyZkAWzbZ/Nef78zWZI0NK70ph4/C6PJM1WSRlL7EN9z2peLaS4HBNi9zyZ7tO2aPuOSJEmbxVAlaZT9vG1fDqxqv++3Ot+Bbbuqz7gkSdJmMVRJmteS7DrN8KK2/XVVPQTcAxyRZOEUc48CHqb/kuuSJEmbxVAlab67PsnHkvzeAhNJdgbObl9e1bYXA7sCSybNPYkmgF3aPghYkiRpi3GhCknz3QrgK8CnkywHVgMvBU6guX/q81V1Wzv3YuA44HNJDgXuAg4CTgRW8vw9WJIkSVuMoUrSvFZVH0tyHfBB4GiaILUBuBv4SFVdN2Hu00neCiwFjm/nrwUuApZV1WODrl+SJG39DFWS5r2qugG4YYZz1wOntV+SJElzznuqJEmShijJ4iR3JlmX5DdJrkyy97Dr0paXZPsky5KsTLIhyRNJbk/yvinmLkhyWpL7kmxMsjrJeUm2G0btmp6hSpIkaUiSfBy4Btge+DxwJXAM8D2D1dYlyWuA/wHOAO4HPgd8FdgLuDzJ6RPmhmYRpnPbuWcBt9EsxHRjkhcNtnq9EC//kyRJGoIkrwAuAL4PvLGqNrT9VwG3Al8G3jG8CrWFHQI8ABxZVc89MzHJBcC9wGeSfLGqNgLvonmw/UVVdcqEuT8AzgdOBS4cZPGanmeqJEmShuPDwEJgaS9QAVTV7cA3gGM8W7VVuQk4fGKgAqiqtcB3aM5W9p6/eDLwFM1ZrYkuBH4NnILmFUOVJEnScLyZZjXTm6YYu75t3zK4cjSXquqBqnqmz/BzoTrJDsDrge9W1aOT9rEJWA7sm2S/OStWs2aokiRJGo6DgJVV9ewUYyva9sAB1qMhaB9ufwRNsFoF7E9zi86KPpv42ZiHDFWSJEkDlmQnYCfgwT5Tev17DaYiDdGpwN7AJe1jQfZs+/1sjBBDlSRJ0uDt2Lbr+oz3+ncYQC0akiSLgHOAXwHL2m4/GyPIUCVJkjR4vb/BNvUZ7/VvM4BaNATt86auplms5MQJ90/52RhBLqkuSZI0eOvbdts+473+fmcrNMLa51BdBhwMfLKqbp0w7GdjBHmmSpIkafAepVkye/c+43u07ZrBlKMBOxs4Hri0qr40aaz3nvvZGCGGKkmSpAGrqt8BPwUO6DOlt7Lbqj7jGlFJ3gucDtwCfHSKKb333M/GCDFUSZIkDcfNwG5JDpli7G0T5mgrkeQw4BLgPmDxVM+tqqqHgHuAI5IsnGI3RwEP03/JdQ2BoUqSJGk4vgYUcG6S5+5zT3Iw8H7grqr60ZBq0xaW5FXAtcCTwNFV9dtppl8M7AosmbSPk4BFNJcN9lvIQkPgQhWSJElDUFX3JLkA+BRwe5JvAbsAHwCeBT4yzPq0xV1B8/5eA7y9WaviD9xRVXfQhKrjgM8lORS4i+Zh0ScCK2mWYdc8YqiSJEkakqr6dJL7gZOBM2hWfrsZOL2q7h1qcdrSegtPHNt+TeUsmmD1dJK3AktpFrQ4GlgLXAQsq6rH5rpYzY6hSpIkaYiq6hKa+2y0FauqfWY5fz1wWvulec57qiRJkiSpA0OVJEmSJHVgqJIkSZKkDgxVkiRJktSBoUqSJEmSOjBUSZIkSVIHhipJkiRJ6sBQJUmSJEkdGKokSZIkqQNDlSRJkiR1YKiSJEmSpA4MVZIkSZLUgaFKkiRJkjpIVQ27BkkaGUkeZuGCnV/00l2GXYo0J/543aZhlzArjzzyCAsWLGDjxo0Zdi2SxpehSpJmIcnPgZ2AX7RdB7TtvUMpaPx4vAdrFI73PsDjVbXvsAuRNL4MVZLUQZK7AarqtcOuZRx4vAfL4y1JM+M9VZIkSZLUgaFKkiRJkjowVEmSJElSB4YqSZIkSerAUCVJkiRJHbj6nyRJkiR14JkqSZIkSerAUCVJkiRJHRiqJEmSJKkDQ5UkSZIkdWCokiRJkqQODFWSJEmS1IGhSpIkSZI6MFRJ0mZKsjjJnUnWJflNkiuT7D3surYGSV6dZG2SSvKmPnMWJDktyX1JNiZZneS8JNsNuNyRlGT7JMuSrEyyIckTSW5P8r4p5nqsJWkaPvxXkjZDko8Dfw/8N/DPwJ8AHwQ2AH9WVauHWN5IS/Ju4MvAzm3X4VV1y6Q5Af4FWAwsB/4TeDVwPHBbu80zg6p51CR5DXAd8DKa4/c94CXAu9u+M6rqnHaux1qSXoChSpJmKckrgJ8BPwbeWFUb2v4/B24FllfVO4ZY4shK8nfABcC1wIPAKUwdqo6jCbMXVdUpE/qXAOcDf1tVFw6q7lGT5P3Ah4CTqmrVhP7dgHuBPwJ2qaqNHmtJemFe/idJs/dhYCGwtBeoAKrqduAbwDFeBrjZ7gP+oqreCTw8zbyTgaeAMyb1Xwj8miaMqb+baMLqqomdVbUW+A6wPbCo7fZYS9ILMFRJ0uy9meYyv5umGLu+bd8yuHK2HlV1fVX923RzkuwAvB74blU9Omn7TTSXqO2bZL+5q3S0VdUD01yy99x/FHisJWlmDFWSNHsHASur6tkpxla07YEDrGfc7A8s4PljPZnvwWZKsg1wBE2wWoXHWpJmxFAlSbOQZCdgJ5r7fabS699rMBWNpT3b1vdgyzsV2Bu4pKrW47GWpBkxVEnS7OzYtuv6jPf6dxhALePK92AOJFkEnAP8CljWdnusJWkGDFWSNDu935ub+oz3+rcZQC3jyvdgC2ufN3U1zQIsJ064f8pjLUkzsGDYBUjSiFnfttv2Ge/19/uffXXne7AFtc+hugw4GPhkVd06YdhjLUkz4JkqSZqdR2mWl969z/gebbtmMOWMpd6x9T3YMs6meZDvpVX1pUljHmtJmgFDlSTNQlX9DvgpcECfKb1V0Fb1GVd3vWPre9BRkvcCpwO3AB+dYorHWpJmwFAlSbN3M7BbkkOmGHvbhDmaA1X1EHAPcESShVNMOYrmwcH9lgEXkOQw4BKaBy4vnuq5VR5rSZoZQ5Ukzd7XgALOTfLcvalJDgbeD9xVVT8aUm3j4mJgV2DJxM4kJwGLaC5l67e4wthL8irgWuBJ4Oiq+u000z3WkvQCUlXDrkGSRk6S84BPAd8HvgXsAnyAZgGgwwxV3SU5E/gscHhV3TJpbCFwE3AY8E3gLpqHMp8I/AR4Q1U9Nsh6R0mSO4HXAdcA/9Vn2h1VdYfHWpJemKFKkjZTkg8BJ9Pcb7Ke5r6U06vq3mHWtbWYLlS149sDS2kWWXg5sJbm7MuyCUuCawpJfkHzkN/pnFVVZ7bzPdaSNA1DlSRJkiR14D1VkiRJktSBoUqSJEmSOjBUSZIkSVIHhipJkiRJ6sBQJUmSJEkdGKokSZIkqQNDlSRJkiR1YKiSJEmSpA4MVZIkSZLUgaFKkiRJkjowVEmSJElSB4YqSZIkSerAUCVJkiRJHRiqJEmSJKkDQ5UkSZIkdWCokiRJkqQODFWSJEmS1IGhSpIkSZI6+H/yLqI5LxNMRwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 504x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 208,
       "width": 426
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch, length = 16, 20\n",
    "src_padding = 5\n",
    "tgt_padding = 15\n",
    "\n",
    "src_pad = tf.zeros(shape=(batch, src_padding))\n",
    "tgt_pad = tf.zeros(shape=(batch, tgt_padding))\n",
    "\n",
    "sample_data = tf.ones(shape=(batch, length))\n",
    "\n",
    "sample_src = tf.concat([sample_data, src_pad], axis=-1)\n",
    "sample_tgt = tf.concat([sample_data, tgt_pad], axis=-1)\n",
    "\n",
    "enc_mask, dec_enc_mask, dec_mask = \\\n",
    "generate_masks(sample_src, sample_tgt)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "\n",
    "ax1.set_title('1) Encoder Mask')\n",
    "ax2.set_title('2) Encoder-Decoder Mask')\n",
    "ax3.set_title('3) Decoder Mask')\n",
    "\n",
    "ax1.imshow(enc_mask[:3, 0, 0].numpy(), cmap='Dark2')\n",
    "ax2.imshow(dec_enc_mask[0, 0].numpy(), cmap='Dark2')\n",
    "ax3.imshow(dec_mask[0, 0].numpy(), cmap='Dark2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "N_LAYERS = 2\n",
    "D_MODEL = 512\n",
    "N_HEADS = 8\n",
    "D_FF = 2048\n",
    "DROPOUT = 0.3\n",
    "\n",
    "transformer = Transformer(\n",
    "    n_layers=N_LAYERS,\n",
    "    d_model=D_MODEL,\n",
    "    n_heads=N_HEADS,\n",
    "    d_ff=D_FF,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=DROPOUT,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step 함수 정의\n",
    "\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # 계산된 loss에 tf.GradientTape()를 적용해 학습을 진행합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    # 최종적으로 optimizer.apply_gradients()가 사용됩니다. \n",
    "    # [[YOUR CODE]]\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#시각화\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "\n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "\n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#번역생성\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#번역과 시각화 결합\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['모닝 커피는 정신 건강에 이롭다.', '즐거운 노래는 분위기를 좋게 한다.', '새로운 사실이 발견됐다.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93bbba63d304b96aa9f8bcebbb59cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=892.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "In[0] mismatch In[1] shape: 50 vs. 1: [1,8,1,50] [1,8,1,64] 0 0 [Op:BatchMatMulV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-94f3fca5d6ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mko_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-e41f80fd2d79>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_attention\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpieces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_attns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_attns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_enc_attns\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-fe3f25e34807>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(sentence, model, src_tokenizer, tgt_tokenizer)\u001b[0m\n\u001b[1;32m     21\u001b[0m               \u001b[0menc_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m               \u001b[0mcombined_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m               dec_padding_mask)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mpredicted_id\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-c9bb0807f690>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mdec_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_attns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_enc_attns\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcausality_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-0ef3ee6d2dd9>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, enc_out, causality_mask, padding_mask)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_enc_attn\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcausality_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mdec_attns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-62944f3df52a>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, enc_out, causality_mask, padding_mask)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec_self_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcausality_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-8bccadd0b576>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, Q, K, V, mask)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         out, attention_weights = self.scaled_dot_product_attention(\n\u001b[0;32m---> 54\u001b[0;31m             WQ_splits, WK_splits, WV_splits, mask)\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-8bccadd0b576>\u001b[0m in \u001b[0;36mscaled_dot_product_attention\u001b[0;34m(self, Q, K, V, mask)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mattentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_qk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   3215\u001b[0m         \u001b[0madjoint_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3216\u001b[0m       return gen_math_ops.batch_mat_mul_v2(\n\u001b[0;32m-> 3217\u001b[0;31m           a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\n\u001b[0m\u001b[1;32m   3218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3219\u001b[0m     \u001b[0;31m# Neither matmul nor sparse_matmul support adjoint, so we conjugate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mbatch_mat_mul_v2\u001b[0;34m(x, y, adj_x, adj_y, name)\u001b[0m\n\u001b[1;32m   1544\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1546\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1547\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: In[0] mismatch In[1] shape: 50 vs. 1: [1,8,1,50] [1,8,1,64] 0 0 [Op:BatchMatMulV2]"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 50\n",
    "EPOCHS = 15\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer, optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "\n",
    "    for sentence in sentences:\n",
    "        translate(sentence, transformer, ko_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('새로운 사실이 발견됐다.',transformer, ko_tokenizer, en_tokenizer, plot_attention = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 루브릭\n",
    "평가문항\n",
    "\t상세기준\n",
    "\n",
    "1. 번역기 모델 학습에 필요한 텍스트 데이터 전처리가 잘 이루어졌다.\n",
    "\t데이터 정제, SentencePiece를 활용한 토큰화 및 데이터셋 구축의 과정이 지시대로 진행되었다.\n",
    "\n",
    "2. Transformer 번역기 모델이 정상적으로 구동된다.\n",
    "\tTransformer 모델의 학습과 추론 과정이 정상적으로 진행되어, 한-영 번역기능이 정상 동작한다.\n",
    "\n",
    "3. 테스트 결과 의미가 통하는 수준의 번역문이 생성되었다.\n",
    "\t제시된 문장에 대한 그럴듯한 영어 번역문이 생성되며, 시각화된 Attention Map으로 결과를 뒷받침한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
