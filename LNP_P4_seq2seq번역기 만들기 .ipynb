{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 목차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. 데이터 정제\n",
    "\n",
    "1) set 데이터형이 중복을 허용하지 않는다는 것을 활용해 중복된 데이터를 제거하도록 합니다. 데이터의 병렬 쌍이 흐트러지지 않게 주의하세요! 중복을 제거한 데이터를 cleaned_corpus 에 저장합니다.\n",
    "\n",
    "2) 앞서 정의한 preprocessing() 함수는 한글에 대해 동작하지 않습니다. 한글에 적용할 수 있는 정규식을 추가하여 함수를 재정의하세요!\n",
    "\n",
    "3) 타겟 언어인 영문엔 <start> 토큰과 <end> 토큰을 추가하고 split() 함수로 토큰화합니다. 한글 토큰화는 KoNLPy의 mecab 클래스를 사용합니다. KoNLPy가 설치되어 있지 않다면 아래 문서를 참고해 설치해 주세요.\n",
    "\n",
    "설치하기-KoNLPy\n",
    "모든 데이터를 사용할 경우 학습에 굉장히 오랜 시간이 걸립니다. cleaned_corpus로부터 토큰의 길이가 40 이하인 데이터를 선별하여 eng_corpus와 kor_corpus를 각각 구축하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "# import matplotlib.font_manager as fm\n",
    "# fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "# font = fm.FontProperties(fname=fontpath, size=9)\n",
    "# plt.rc('font', family='NanumBarunGothic') \n",
    "# mpl.font_manager._rebuild()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 94123\n",
      "Example:\n",
      ">> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",
      ">> 북한의 핵무기 계획을 포기하도록 하려는 압력이 거세지고 있는 가운데, 일본과 북한의 외교관들이 외교 관계를 정상화하려는 회담을 재개했다.\n",
      ">> \"경호 로보트가 침입자나 화재를 탐지하기 위해서 개인적으로, 그리고 전문적으로 사용되고 있습니다.\"\n",
      ">> 수자원부 당국은 논란이 되고 있고, 막대한 비용이 드는 이 사업에 대해 내년에 건설을 시작할 계획이다.\n",
      ">> 또한 근력 운동은 활발하게 걷는 것이나 최소한 20분 동안 뛰는 것과 같은 유산소 활동에서 얻는 운동 효과를 심장과 폐에 주지 않기 때문에, 연구학자들은 근력 운동이 심장에 큰 영향을 미치는지 여부에 대해 논쟁을 해왔다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path_to_file = os.getenv('HOME')+'/aiffel/aiffel_nlp_ex/data/korean-english-park.train.ko'\n",
    "\n",
    "with open(path_to_file, \"r\") as f:\n",
    "    train_ko_raw = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(train_ko_raw))\n",
    "\n",
    "print(\"Example:\")\n",
    "for sen in train_ko_raw[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 94123\n",
      "Example:\n",
      ">> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",
      ">> 북한의 핵무기 계획을 포기하도록 하려는 압력이 거세지고 있는 가운데, 일본과 북한의 외교관들이 외교 관계를 정상화하려는 회담을 재개했다.\n",
      ">> \"경호 로보트가 침입자나 화재를 탐지하기 위해서 개인적으로, 그리고 전문적으로 사용되고 있습니다.\"\n",
      ">> 수자원부 당국은 논란이 되고 있고, 막대한 비용이 드는 이 사업에 대해 내년에 건설을 시작할 계획이다.\n",
      ">> 또한 근력 운동은 활발하게 걷는 것이나 최소한 20분 동안 뛰는 것과 같은 유산소 활동에서 얻는 운동 효과를 심장과 폐에 주지 않기 때문에, 연구학자들은 근력 운동이 심장에 큰 영향을 미치는지 여부에 대해 논쟁을 해왔다.\n"
     ]
    }
   ],
   "source": [
    "path_to_file = os.getenv('HOME')+'/aiffel/aiffel_nlp_ex/data/korean-english-park.train.ko'\n",
    "\n",
    "with open(path_to_file, \"r\") as f:\n",
    "    train_en_raw = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(train_ko_raw))\n",
    "\n",
    "print(\"Example:\")\n",
    "for sen in train_ko_raw[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = list(set(zip(train_ko_raw, train_en_raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77591"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_preprocess_sentence(sentence, s_token=False, e_token=False):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "    \n",
    "    sentence = sentence.split() #영어 토큰화\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ko_preprocess_sentence(sentence, s_token=False, e_token=False):\n",
    "    mecab = Mecab()\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z?.!,1-9\\\\s]\", \"\", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "    \n",
    "    sentence = mecab.morphs(sentence) #한글 토큰화\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "ko_corpus = []\n",
    "en_corpus = []\n",
    "\n",
    "num_examples = 30000\n",
    "\n",
    "for i in range(len(cleaned_corpus)):\n",
    "    en_sentence = en_preprocess_sentence(cleaned_corpus[i][1], s_token = True, e_token = True)\n",
    "    ko_sentence = ko_preprocess_sentence(cleaned_corpus[i][0])\n",
    "    if len(en_sentence) <=20 and len(ko_sentence) <=20:\n",
    "        en_corpus.append(en_sentence)\n",
    "        ko_corpus.append(ko_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17345\n",
      "17345\n"
     ]
    }
   ],
   "source": [
    "print(len(en_corpus))\n",
    "print(len(ko_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. 데이터 토큰화\n",
    "\n",
    "앞서 정의한 tokenize() 함수를 사용해 데이터를 텐서로 변환하고 각각의 tokenizer를 얻으세요! 단어의 수는 실험을 통해 적당한 값을 맞춰주도록 합니다! (최소 10,000 이상!)\n",
    "\n",
    "난이도에 비해 데이터가 많지 않아 훈련 데이터와 검증 데이터를 따로 나누지는 않습니다.₩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean Vocab Size: 21713\n",
      "English Vocab size: 3579\n"
     ]
    }
   ],
   "source": [
    "# 토큰화하기\n",
    "enc_tensor, enc_tokenizer = tokenize(ko_corpus)\n",
    "dec_tensor, dec_tokenizer = tokenize(en_corpus)\n",
    "\n",
    "print('Korean Vocab Size:', len(enc_tokenizer.index_word))\n",
    "print('English Vocab size:', len(dec_tokenizer.index_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. 모델 설계\n",
    "\n",
    "한국어를 영어로 잘 번역해 줄 멋진 Attention 기반 Seq2seq 모델을 설계하세요! 앞서 만든 모델에 Dropout 모듈을 추가하면 성능이 더 좋아질 거랍니다! Embedding Size와 Hidden Size는 실험을 통해 적당한 값을 맞춰 주도록 합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units,\n",
    "                                       return_sequences=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.gru(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
    "\n",
    "        out, h_dec = self.gru(out)\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output: (64, 30, 1024)\n",
      "Decoder Output: (64, 3580)\n",
      "Decoder Hidden State: (64, 1024)\n",
      "Attention: (64, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE     = 64\n",
    "SRC_VOCAB_SIZE = len(enc_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(dec_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 1024\n",
    "embedding_dim = 512\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5. 훈련하기\n",
    "\n",
    "훈련엔 위에서 사용한 코드를 그대로 사용하되, eval_step() 부분이 없음에 유의합니다! 매 스텝 아래의 예문에 대한 번역을 생성하여 본인이 생각하기에 가장 멋지게 번역한 Case를 제출하세요! (Attention Map을 시각화해보는 것도 재밌을 거예요!)\n",
    "\n",
    "참고: 데이터의 난이도가 높은 편이므로 생각만큼 결과가 잘 안나올 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_out = encoder(src)\n",
    "        h_dec = enc_out[:, -1]\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "            loss += loss_function(tgt[:, t], pred)\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 272/272 [06:37<00:00,  1.46s/it, Loss 0.2882]\n",
      "Epoch  2: 100%|██████████| 272/272 [06:17<00:00,  1.39s/it, Loss 0.1945]\n",
      "Epoch  3: 100%|██████████| 272/272 [06:19<00:00,  1.39s/it, Loss 0.1631]\n",
      "Epoch  4: 100%|██████████| 272/272 [06:17<00:00,  1.39s/it, Loss 0.1389]\n",
      "Epoch  5: 100%|██████████| 272/272 [06:13<00:00,  1.37s/it, Loss 0.1124]\n",
      "Epoch  6: 100%|██████████| 272/272 [06:00<00:00,  1.33s/it, Loss 0.0876]\n",
      "Epoch  7: 100%|██████████| 272/272 [06:00<00:00,  1.33s/it, Loss 0.0643]\n",
      "Epoch  8: 100%|██████████| 272/272 [06:34<00:00,  1.45s/it, Loss 0.0439]\n",
      "Epoch  9: 100%|██████████| 272/272 [06:40<00:00,  1.47s/it, Loss 0.0287]\n",
      "Epoch 10: 100%|██████████| 272/272 [06:51<00:00,  1.51s/it, Loss 0.0184]\n",
      "Epoch 11: 100%|██████████| 272/272 [07:00<00:00,  1.55s/it, Loss 0.0125]\n",
      "Epoch 12: 100%|██████████| 272/272 [06:59<00:00,  1.54s/it, Loss 0.0092]\n",
      "Epoch 13: 100%|██████████| 272/272 [06:43<00:00,  1.48s/it, Loss 0.0071]\n",
      "Epoch 14: 100%|██████████| 272/272 [06:41<00:00,  1.48s/it, Loss 0.0060]\n",
      "Epoch 15: 100%|██████████| 272/272 [06:48<00:00,  1.50s/it, Loss 0.0064]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 15\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_tensor.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(enc_tensor[idx:idx+BATCH_SIZE],\n",
    "                                dec_tensor[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((dec_tensor.shape[-1], enc_tensor.shape[-1]))\n",
    "    \n",
    "    sentence = ko_preprocess_sentence(sentence)\n",
    "    inputs = enc_tokenizer.texts_to_sequences([sentence])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=enc_tensor.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(dec_tensor.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention\n",
    "\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention = attention[:len(result.split()), :len(sentence)]\n",
    "    plot_attention(attention, sentence, result.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 50500 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 52840 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 48165 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 47673 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 50612 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 46972 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 50500 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 52840 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 48165 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:176: RuntimeWarning: Glyph 47673 missing from current font.\n",
      "  font.load_char(ord(s), flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 50612 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 46972 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['아침밥', '먹', '어라', '.']\n",
      "Predicted translation: . <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(\"아침밥 먹어라.\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['시간', '이', '약', '이', '다', '.']\n",
      "Predicted translation: . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 49884 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 44036 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 51060 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 50557 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 45796 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 49884 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 44036 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:176: RuntimeWarning: Glyph 51060 missing from current font.\n",
      "  font.load_char(ord(s), flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:176: RuntimeWarning: Glyph 50557 missing from current font.\n",
      "  font.load_char(ord(s), flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:176: RuntimeWarning: Glyph 45796 missing from current font.\n",
      "  font.load_char(ord(s), flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(\"시간이 약이다.\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['커피', '는', '아메리카노', '.']\n",
      "Predicted translation: . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 52964 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 54588 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 45716 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 47700 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 47532 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 52852 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 45432 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 52964 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 54588 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:176: RuntimeWarning: Glyph 45716 missing from current font.\n",
      "  font.load_char(ord(s), flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 47700 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 47532 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 52852 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/aiffel-dj54/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 45432 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(\"커피는 아메리카노.\", encoder, decoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
