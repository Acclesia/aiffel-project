{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "geographic-discharge",
   "metadata": {},
   "source": [
    "# 루브릭\n",
    "평가문항\t상세기준\n",
    "\n",
    "1. SRGAN을 이용해 고해상도의 이미지를 생성하였다.\n",
    "\tSRGAN을 통해 생성된 이미지를 제출하였다.\n",
    "\n",
    "2. 다양한 해상도의 이미지에 대해 시각화를 통해 원본, SRGAN생성본, interpolation생성본을 비교분석하였다.\n",
    "\t이미지의 특성과 super resolution 방법을 관련지어 생성 결과를 체계적으로 분석하였다.\n",
    "\n",
    "3. 저해상도 gif 동영상을 고해상도 동영상으로 성공적으로 변환하였다.\n",
    "\t저해상도 원본 gif와 생성된 고해상도 gif의 해상도 차이가 시각적으로 확인 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-commerce",
   "metadata": {},
   "source": [
    "# 프로젝트 1 : 직접 고른 이미지로 SRGAN 실험하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-price",
   "metadata": {},
   "source": [
    "프로젝트 1-1.\n",
    "\n",
    "    (적당히) 높은 해상도를 가진 이미지를 검색해서 한 장 고른 후 저장하고 불러옵니다.\n",
    "    불러온 이미지에 bicubic interpolation을 적용해 가로 및 세로 픽셀 수를 1/4로 줄입니다. cv2.resize()를 사용해 봅시다.\n",
    "    줄인 저해상도 이미지를 입력으로 SRGAN을 이용해 고해상도 이미지를 생성합니다. 이전에 사용한 apply_srgan 함수를 사용하면 쉽습니다.\n",
    "    2.의 이미지에 bicubic interpolation을 적용해 가로 및 세로 픽셀 수를 다시 4배로 늘립니다. 마찬가지로 cv2.resize()를 사용해 봅시다.\n",
    "    3개 이미지(4.의 Bicubic의 결과, 3.의 SRGAN의 결과, 1.의 원래 고해상도 이미지)를 나란히 시각화합니다. 각 이미지의 제목에 어떤 방법에 대한 결과인지 표시해 주세요. 이전 시각화에 사용했던 코드를 참고하면 어렵지 않습니다.\n",
    "    선택한 이미지를 DIV2K 데이터셋에서 학습된 모델로 Super Resolution했을 때 어떠한 결과가 나왔으며, 왜 이러한 결과가 출력되었는지 설명해 봅시다. (정답은 없습니다)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "incorporate-thumbnail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-datasets in /home/aiffel-dj54/anaconda3/envs/aiffel/lib/python3.7/site-packages (4.2.0)\n",
      "Requirement already satisfied: promise in /home/aiffel-dj54/anaconda3/envs/aiffel/lib/python3.7/site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /home/aiffel-dj54/anaconda3/envs/aiffel/lib/python3.7/site-packages (from tensorflow-datasets) (3.13.0)\n",
      "Requirement already satisfied: tqdm in /home/aiffel-dj54/anaconda3/envs/aiffel/lib/python3.7/site-packages (from tensorflow-datasets) (4.56.0)\n",
      "Requirement already satisfied: termcolor in /home/aiffel-dj54/anaconda3/envs/aiffel/lib/python3.7/site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /home/aiffel-dj54/anaconda3/envs/aiffel/lib/python3.7/site-packages (from tensorflow-datasets) (20.3.0)\n",
      "Requirement already satisfied: absl-py in /home/aiffel-dj54/anaconda3/envs/aiffel/lib/python3.7/site-packages (from tensorflow-datasets) (0.10.0)\n",
      "Requirement already satisfied: six in /home/aiffel-dj54/anaconda3/envs/aiffel/lib/python3.7/site-packages (from tensorflow-datasets) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions in /home/aiffel-dj54/anaconda3/envs/aiffel/lib/python3.7/site-packages (from tensorflow-datasets) (3.7.4.3)\n",
      "Requirement already satisfied: tensorflow-metadata in /home/aiffel-dj54/anaconda3/envs/aiffel/lib/python3.7/site-packages (from tensorflow-datasets) (0.28.0)\n",
      "Requirement already satisfied: importlib-resources in /home/aiffel-dj54/anaconda3/envs/aiffel/lib/python3.7/site-packages (from tensorflow-datasets) (5.1.1)\n",
      "Requirement already satisfied: numpy in /home/aiffel-dj54/anaconda3/envs/aiffel/lib/python3.7/site-packages (from tensorflow-datasets) (1.19.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/aiffel-dj54/anaconda3/envs/aiffel/lib/python3.7/site-packages (from tensorflow-datasets) (2.25.1)\n",
      "Requirement already satisfied: dill in /home/aiffel-dj54/anaconda3/envs/aiffel/lib/python3.7/site-packages (from tensorflow-datasets) (0.3.3)\n",
      "Requirement already satisfied: future in /home/aiffel-dj54/anaconda3/envs/aiffel/lib/python3.7/site-packages (from tensorflow-datasets) (0.18.2)\n",
      "Requirement already satisfied: setuptools in /home/aiffel-dj54/anaconda3/envs/aiffel/lib/python3.7/site-packages (from protobuf>=3.12.2->tensorflow-datasets) (52.0.0.post20210125)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/aiffel-dj54/anaconda3/envs/aiffel/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/aiffel-dj54/anaconda3/envs/aiffel/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aiffel-dj54/anaconda3/envs/aiffel/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/aiffel-dj54/anaconda3/envs/aiffel/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.4 in /home/aiffel-dj54/anaconda3/envs/aiffel/lib/python3.7/site-packages (from importlib-resources->tensorflow-datasets) (3.4.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /home/aiffel-dj54/anaconda3/envs/aiffel/lib/python3.7/site-packages (from tensorflow-metadata->tensorflow-datasets) (1.53.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "neutral-tours",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cheap-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tfdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-table",
   "metadata": {},
   "source": [
    "프로젝트 1-2.\n",
    "\n",
    "    (적당히) 낮은 해상도를 가진 이미지를 검색해서 한 장 고른 후 저장하고 불러옵니다.\n",
    "    불러온 이미지를 입력으로 SRGAN을 이용해 고해상도 이미지를 생성합니다. 이전에 사용한 apply_srgan 함수를 사용하면 쉽습니다.\n",
    "    1.에서 불러온 이미지에 bicubic interpolation을 적용해 가로 및 세로 픽셀 수를 다시 4배로 늘립니다. cv2.resize()를 사용해 봅시다.\n",
    "    2개 이미지(3.의 Bicubic의 결과, 2.의 SRGAN의 결과)를 나란히 시각화합니다. 각 이미지의 제목에 어떤 방법에 대한 결과인지 표시해 주세요. 이전 시각화에 사용했던 코드를 참고하면 어렵지 않습니다.\n",
    "    선택한 이미지를 DIV2K 데이터셋에서 학습된 모델로 Super Resolution했을 때 어떠한 결과가 나왔으며, 왜 이러한 결과가 출력되었는지 설명해 봅시다. (정답은 없습니다)\n",
    "\n",
    "    프로젝트 2 : SRGAN을 이용해 고해상도 gif 생성하기\n",
    "\n",
    "이전 Super Resolution의 활용 사례에 대해 살펴봤을 때, 단일 이미지가 아닌 영상에 대해 Super Resolution을 적용한 사례가 있었습니다. 이번에는 이미 학습된 SRGAN을 이용해 저해상도 영상을 고해상도 영상으로 바꿔보는 프로젝트를 수행해 봅시다.\n",
    "\n",
    "실제로 동영상의 Super Resolution은 시간 순서에 따른 다수의 프레임 정보를 고려하는 것이 더 좋지만, 처음부터 학습시키기에 많은 시간이 소요됩니다. 여기서는 이전에 사용했던 SRGAN을 이용해 한 프레임씩 고해상도 이미지로 변환 시켜 모든 프레임에 대해 적용하고, 그 프레임들을 합쳐 동영상으로 만들어 봅시다.\n",
    "\n",
    "각 프레임들을 모아 gif 파일을 만드는데 아래 라이브러리의 설치가 필요합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecological-welding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅\n"
     ]
    }
   ],
   "source": [
    "#저해상도 이미지를 받아 고해상도 이미지를 생성\n",
    "from tensorflow.keras import Input, Model\n",
    "\n",
    "# 그림의 파란색 블록을 정의합니다.\n",
    "def gene_base_block(x):\n",
    "    out = layers.Conv2D(64, 3, 1, \"same\")(x)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.PReLU(shared_axes=[1,2])(out)\n",
    "    out = layers.Conv2D(64, 3, 1, \"same\")(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    return layers.Add()([x, out])\n",
    "\n",
    "# 그림의 뒤쪽 연두색 블록을 정의합니다.\n",
    "def upsample_block(x):\n",
    "    out = layers.Conv2D(256, 3, 1, \"same\")(x)\n",
    "    # 그림의 PixelShuffler 라고 쓰여진 부분을 아래와 같이 구현합니다.\n",
    "    out = layers.Lambda(lambda x: tf.nn.depth_to_space(x, 2))(out)\n",
    "    return layers.PReLU(shared_axes=[1,2])(out)\n",
    "    \n",
    "# 전체 Generator를 정의합니다.\n",
    "def get_generator(input_shape=(None, None, 3)):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    out = layers.Conv2D(64, 9, 1, \"same\")(inputs)\n",
    "    out = residual = layers.PReLU(shared_axes=[1,2])(out)\n",
    "    \n",
    "    for _ in range(5):\n",
    "        out = gene_base_block(out)\n",
    "    \n",
    "    out = layers.Conv2D(64, 3, 1, \"same\")(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Add()([residual, out])\n",
    "    \n",
    "    for _ in range(2):\n",
    "        out = upsample_block(out)\n",
    "        \n",
    "    out = layers.Conv2D(3, 9, 1, \"same\", activation=\"tanh\")(out)\n",
    "    return Model(inputs, out)\n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "instrumental-plant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅\n"
     ]
    }
   ],
   "source": [
    "#고해상도 이미지와 진짜 고해상도 이미지 판별하는discriminator\n",
    "# 그림의 파란색 블록을 정의합니다.\n",
    "def disc_base_block(x, n_filters=128):\n",
    "    out = layers.Conv2D(n_filters, 3, 1, \"same\")(x)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.LeakyReLU()(out)\n",
    "    out = layers.Conv2D(n_filters, 3, 2, \"same\")(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    return layers.LeakyReLU()(out)\n",
    "\n",
    "# 전체 Discriminator 정의합니다.\n",
    "def get_discriminator(input_shape=(None, None, 3)):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    out = layers.Conv2D(64, 3, 1, \"same\")(inputs)\n",
    "    out = layers.LeakyReLU()(out)\n",
    "    out = layers.Conv2D(64, 3, 2, \"same\")(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.LeakyReLU()(out)\n",
    "    \n",
    "    for n_filters in [128, 256, 512]:\n",
    "        out = disc_base_block(out, n_filters)\n",
    "    \n",
    "    out = layers.Dense(1024)(out)\n",
    "    out = layers.LeakyReLU()(out)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(out)\n",
    "    return Model(inputs, out)\n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dutch-manitoba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅\n"
     ]
    }
   ],
   "source": [
    "#content loss 계산\n",
    "from tensorflow.python.keras import applications\n",
    "def get_feature_extractor(input_shape=(None, None, 3)):\n",
    "    vgg = applications.vgg19.VGG19(\n",
    "        include_top=False, \n",
    "        weights=\"imagenet\", \n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    # 아래 vgg.layers[20]은 vgg 내의 마지막 convolutional layer 입니다.\n",
    "    return Model(vgg.input, vgg.layers[20].output)\n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "qualified-combat",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-b68fa008391c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvgg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_feature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-76d70e7a78af>\u001b[0m in \u001b[0;36mget_generator\u001b[0;34m(input_shape)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"same\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshared_axes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"
     ]
    }
   ],
   "source": [
    "# srgan 학습하기\n",
    "from tensorflow.keras import losses, metrics, optimizers\n",
    "\n",
    "generator = get_generator()\n",
    "discriminator = get_discriminator()\n",
    "vgg = get_feature_extractor()\n",
    "\n",
    "# 사용할 loss function 및 optimizer 를 정의합니다.\n",
    "bce = losses.BinaryCrossentropy(from_logits=False)\n",
    "mse = losses.MeanSquaredError()\n",
    "gene_opt = optimizers.Adam()\n",
    "disc_opt = optimizers.Adam()\n",
    "\n",
    "def get_gene_loss(fake_out):\n",
    "    return bce(tf.ones_like(fake_out), fake_out)\n",
    "\n",
    "def get_disc_loss(real_out, fake_out):\n",
    "    return bce(tf.ones_like(real_out), real_out) + bce(tf.zeros_like(fake_out), fake_out)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def get_content_loss(hr_real, hr_fake):\n",
    "    hr_real = applications.vgg19.preprocess_input(hr_real)\n",
    "    hr_fake = applications.vgg19.preprocess_input(hr_fake)\n",
    "    \n",
    "    hr_real_feature = vgg(hr_real) / 12.75\n",
    "    hr_fake_feature = vgg(hr_fake) / 12.75\n",
    "    return mse(hr_real_feature, hr_fake_feature)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def step(lr, hr_real):\n",
    "    with tf.GradientTape() as gene_tape, tf.GradientTape() as disc_tape:\n",
    "        hr_fake = generator(lr, training=True)\n",
    "        \n",
    "        real_out = discriminator(hr_real, training=True)\n",
    "        fake_out = discriminator(hr_fake, training=True)\n",
    "        \n",
    "        perceptual_loss = get_content_loss(hr_real, hr_fake) + 1e-3 * get_gene_loss(fake_out)\n",
    "        discriminator_loss = get_disc_loss(real_out, fake_out)\n",
    "        \n",
    "    gene_gradient = gene_tape.gradient(perceptual_loss, generator.trainable_variables)\n",
    "    disc_gradient = disc_tape.gradient(discriminator_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    gene_opt.apply_gradients(zip(gene_gradient, generator.trainable_variables))\n",
    "    disc_opt.apply_gradients(zip(disc_gradient, discriminator.trainable_variables))\n",
    "    return perceptual_loss, discriminator_loss\n",
    "\n",
    "\n",
    "gene_losses = metrics.Mean()\n",
    "disc_losses = metrics.Mean()\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    for i, (lr, hr) in enumerate(train):\n",
    "        g_loss, d_loss = step(lr, hr)\n",
    "        \n",
    "        gene_losses.update_state(g_loss)\n",
    "        disc_losses.update_state(d_loss)\n",
    "        \n",
    "        # 10회 반복마다 loss를 출력합니다.\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"EPOCH[{epoch}] - STEP[{i+1}] \\nGenerator_loss:{gene_losses.result():.4f} \\nDiscriminator_loss:{disc_losses.result():.4f}\", end=\"\\n\\n\")\n",
    "        \n",
    "        if (i+1) == 200:\n",
    "            break\n",
    "            \n",
    "    gene_losses.reset_states()\n",
    "    disc_losses.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-ground",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-rough",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
